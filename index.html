<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta charset="utf-8">
<meta name="generator" content="ReSpec 32.1.4">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<style>
.issue-label{text-transform:initial}
.warning>p:first-child{margin-top:0}
.warning{padding:.5em;border-left-width:.5em;border-left-style:solid}
span.warning{padding:.1em .5em .15em}
.issue.closed span.issue-number{text-decoration:line-through}
.warning{border-color:#f11;border-width:.2em;border-style:solid;background:#fbe9e9}
.warning-title:before{content:"⚠";font-size:1.3em;float:left;padding-right:.3em;margin-top:-.3em}
li.task-list-item{list-style:none}
input.task-list-item-checkbox{margin:0 .35em .25em -1.6em;vertical-align:middle}
.issue a.respec-gh-label{padding:5px;margin:0 2px 0 2px;font-size:10px;text-transform:none;text-decoration:none;font-weight:700;border-radius:4px;position:relative;bottom:2px;border:none;display:inline-block}
</style>
<style>
dfn{cursor:pointer}
.dfn-panel{position:absolute;z-index:35;min-width:300px;max-width:500px;padding:.5em .75em;margin-top:.6em;font:small Helvetica Neue,sans-serif,Droid Sans Fallback;background:#fff;color:#000;box-shadow:0 1em 3em -.4em rgba(0,0,0,.3),0 0 1px 1px rgba(0,0,0,.05);border-radius:2px}
.dfn-panel:not(.docked)>.caret{position:absolute;top:-9px}
.dfn-panel:not(.docked)>.caret::after,.dfn-panel:not(.docked)>.caret::before{content:"";position:absolute;border:10px solid transparent;border-top:0;border-bottom:10px solid #fff;top:0}
.dfn-panel:not(.docked)>.caret::before{border-bottom:9px solid #a2a9b1}
.dfn-panel *{margin:0}
.dfn-panel b{display:block;color:#000;margin-top:.25em}
.dfn-panel ul a[href]{color:#333}
.dfn-panel>div{display:flex}
.dfn-panel a.self-link{font-weight:700;margin-right:auto}
.dfn-panel .marker{padding:.1em;margin-left:.5em;border-radius:.2em;text-align:center;white-space:nowrap;font-size:90%;color:#040b1c}
.dfn-panel .marker.dfn-exported{background:#d1edfd;box-shadow:0 0 0 .125em #1ca5f940}
.dfn-panel .marker.idl-block{background:#8ccbf2;box-shadow:0 0 0 .125em #0670b161}
.dfn-panel a:not(:hover){text-decoration:none!important;border-bottom:none!important}
.dfn-panel a[href]:hover{border-bottom-width:1px}
.dfn-panel ul{padding:0}
.dfn-panel li{margin-left:1em}
.dfn-panel.docked{position:fixed;left:.5em;top:unset;bottom:2em;margin:0 auto;max-width:calc(100vw - .75em * 2 - .5em - .2em * 2);max-height:30vh;overflow:auto}
</style>
		
		
<title>XR Accessibility User Requirements</title>
		
<style id="respec-mainstyle">
@keyframes pop{
0%{transform:scale(1,1)}
25%{transform:scale(1.25,1.25);opacity:.75}
100%{transform:scale(1,1)}
}
:is(h1,h2,h3,h4,h5,h6,a) abbr{border:none}
dfn{font-weight:700}
a.internalDFN{color:inherit;border-bottom:1px solid #99c;text-decoration:none}
a.externalDFN{color:inherit;border-bottom:1px dotted #ccc;text-decoration:none}
a.bibref{text-decoration:none}
.respec-offending-element:target{animation:pop .25s ease-in-out 0s 1}
.respec-offending-element,a[href].respec-offending-element{text-decoration:red wavy underline}
@supports not (text-decoration:red wavy underline){
.respec-offending-element:not(pre){display:inline-block}
.respec-offending-element{background:url(data:image/gif;base64,R0lGODdhBAADAPEAANv///8AAP///wAAACwAAAAABAADAEACBZQjmIAFADs=) bottom repeat-x}
}
#references :target{background:#eaf3ff;animation:pop .4s ease-in-out 0s 1}
cite .bibref{font-style:normal}
code{color:#c63501}
th code{color:inherit}
a[href].orcid{padding-left:4px;padding-right:4px}
a[href].orcid>svg{margin-bottom:-2px}
.toc a,.tof a{text-decoration:none}
a .figno,a .secno{color:#000}
ol.tof,ul.tof{list-style:none outside none}
.caption{margin-top:.5em;font-style:italic}
table.simple{border-spacing:0;border-collapse:collapse;border-bottom:3px solid #005a9c}
.simple th{background:#005a9c;color:#fff;padding:3px 5px;text-align:left}
.simple th a{color:#fff;padding:3px 5px;text-align:left}
.simple th[scope=row]{background:inherit;color:inherit;border-top:1px solid #ddd}
.simple td{padding:3px 10px;border-top:1px solid #ddd}
.simple tr:nth-child(even){background:#f0f6ff}
.section dd>p:first-child{margin-top:0}
.section dd>p:last-child{margin-bottom:0}
.section dd{margin-bottom:1em}
.section dl.attrs dd,.section dl.eldef dd{margin-bottom:0}
#issue-summary>ul{column-count:2}
#issue-summary li{list-style:none;display:inline-block}
details.respec-tests-details{margin-left:1em;display:inline-block;vertical-align:top}
details.respec-tests-details>*{padding-right:2em}
details.respec-tests-details[open]{z-index:999999;position:absolute;border:thin solid #cad3e2;border-radius:.3em;background-color:#fff;padding-bottom:.5em}
details.respec-tests-details[open]>summary{border-bottom:thin solid #cad3e2;padding-left:1em;margin-bottom:1em;line-height:2em}
details.respec-tests-details>ul{width:100%;margin-top:-.3em}
details.respec-tests-details>li{padding-left:1em}
.self-link:hover{opacity:1;text-decoration:none;background-color:transparent}
aside.example .marker>a.self-link{color:inherit}
.header-wrapper{display:flex;align-items:baseline}
:is(h2,h3,h4,h5,h6):not(#toc>h2,#abstract>h2,#sotd>h2,.head>h2){position:relative;left:-.5em}
:is(h2,h3,h4,h5,h6):not(#toch2)+a.self-link{color:inherit;order:-1;position:relative;left:-1.1em;font-size:1rem;opacity:.5}
:is(h2,h3,h4,h5,h6)+a.self-link::before{content:"§";text-decoration:none;color:var(--heading-text)}
:is(h2,h3)+a.self-link{top:-.2em}
:is(h4,h5,h6)+a.self-link::before{color:#000}
@media (max-width:767px){
dd{margin-left:0}
}
@media print{
.removeOnSave{display:none}
}
</style>
		
		
	
<meta name="description" content="This document lists user needs and requirements for people with disabilities when using virtual reality or immersive environments, augmented or mixed reality and other related technologies (XR). It first introduces a definition of XR as used throughout the document, then briefly outlines some uses of XR. It outlines the complexity of understanding XR, introduces some technical accessibility challenges such as the need for multi-modal support, synchronization of input and output devices and customization.  It then outlines accessibility related user needs for XR and suggests subsequent requirements. This is followed by related work that may be helpful understanding the complex technical architecture and processes behind how XR environments are built and what may form the basis of a robust accessibility architecture.">
<style>
var{position:relative;cursor:pointer}
var[data-type]::after,var[data-type]::before{position:absolute;left:50%;top:-6px;opacity:0;transition:opacity .4s;pointer-events:none}
var[data-type]::before{content:"";transform:translateX(-50%);border-width:4px 6px 0 6px;border-style:solid;border-color:transparent;border-top-color:#000}
var[data-type]::after{content:attr(data-type);transform:translateX(-50%) translateY(-100%);background:#000;text-align:center;font-family:"Dank Mono","Fira Code",monospace;font-style:normal;padding:6px;border-radius:3px;color:#daca88;text-indent:0;font-weight:400}
var[data-type]:hover::after,var[data-type]:hover::before{opacity:1}
</style>
<script id="initialUserConfig" type="application/json">{
  "trace": true,
  "useExperimentalStyles": true,
  "doRDFa": "1.1",
  "includePermalinks": true,
  "permalinkEdge": true,
  "permalinkHide": false,
  "noRecTrack": true,
  "tocIntroductory": true,
  "specStatus": "ED",
  "diffTool": "http://www.aptest.com/standards/htmldiff/htmldiff.pl",
  "shortName": "xaur",
  "copyrightStart": "2020",
  "license": "w3c-software-doc",
  "edDraftURI": "https://w3c.github.io/xaur/",
  "github": "w3c/xaur",
  "editors": [
    {
      "name": "Joshue O'Connor",
      "mailto": "joconnor@w3.org",
      "url": "mailto:joconnor@w3.org",
      "company": "W3C",
      "companyURI": "https://www.w3.org",
      "w3cid": 41218
    },
    {
      "name": "Janina Sajka",
      "url": "mailto:janina@rednote.net",
      "mailto": "janina@rednote.net",
      "w3cid": 33688
    },
    {
      "name": "Jason White",
      "url": "mailto:jjwhite@ets.org",
      "company": "Educational Testing Service",
      "mailto": "jjwhite@ets.org",
      "companyURI": "http://www.ets.org/",
      "w3cid": 74028
    },
    {
      "name": "Scott Hollier",
      "url": "mailto:scott@hollier.info",
      "mailto": "scott@hollier.info"
    },
    {
      "name": "Michael Cooper",
      "url": "mailto:cooper@w3.org",
      "mailto": "cooper@w3.org",
      "company": "W3C",
      "companyURI": "https://www.w3.org",
      "w3cid": 34017
    }
  ],
  "group": "wai-apa",
  "maxTocLevel": 4,
  "localBiblio": {
    "aicaptcha": {
      "title": "aiCaptcha: Using AI to beat CAPTCHA and post comment spam",
      "date": "",
      "authors": [
        "Casey Chesnut"
      ],
      "editors": [],
      "etAl": false,
      "publisher": "",
      "href": "http://www.brains-n-brawn.com/default.aspx?vDir=aicaptcha"
    },
    "able-gamers": {
      "title": "Thought On Accessibility and VR",
      "date": "March, 2017",
      "authors": [
        "AJ Ryan"
      ],
      "editors": [],
      "etAl": false,
      "publisher": "",
      "href": "https://ablegamers.org/thoughts-on-accessibility-and-vr/",
      "id": "able-gamers"
    },
    "antiphishing": {
      "title": "Phishing Activity Trends Report",
      "date": "July, 2005",
      "authors": [],
      "editors": [],
      "etAl": false,
      "publisher": "Anti-Phishing Working Group",
      "href": "http://antiphishing.org/APWG_Phishing_Activity_Report_Jul_05.pdf"
    },
    "breaking": {
      "title": "Breaking CAPTCHAs Without Using OCR",
      "date": "",
      "authors": [
        "Howard Yeend"
      ],
      "editors": [],
      "etAl": false,
      "publisher": "",
      "href": "http://www.cs.berkeley.edu/%7Emori/gimpy/gimpy.html"
    },
    "breakingocr": {
      "title": "Breaking CAPTCHAs Without Using OCR",
      "date": "",
      "authors": [
        "Howard Yeend"
      ],
      "editors": [],
      "etAl": false,
      "publisher": "",
      "href": "http://www.puremango.co.uk/cm_breaking_captcha_115.php"
    },
    "chafee": {
      "title": "17 USC 121, Limitations on exclusive rights: reproduction for blind or other people with disabilities (also known as the Chafee Amendment)",
      "date": "",
      "authors": [],
      "editors": [],
      "etAl": false,
      "publisher": "",
      "href": "https://www.copyright.gov/title17/92chap1.html"
    },
    "captcha-ocr": {
      "title": "CAPTCHA: Attacks and Weaknesses against OCR technology",
      "date": "2013",
      "authors": [
        "Silky Azad",
        "Kiran Jain"
      ],
      "publisher": "Journal of Computer science and Technology",
      "etAl": false,
      "editors": [],
      "href": "https://computerresearch.org/index.php/computer/article/download/368/368"
    },
    "information-security": {
      "title": "Handbook of Information and Communication Security",
      "authors": [
        "Peter Stavroulakis",
        "Mark Stamp"
      ],
      "publisher": "Springer Science &amp; Business Media",
      "date": "2010"
    },
    "kaPoW-plugins": {
      "title": "kaPoW plugins: protecting web applications using reputation-based proof-of-work",
      "date": "2012",
      "authors": [
        "Tien Le",
        "Akshay Dua",
        "Wu-chang Feng"
      ],
      "editors": [],
      "etAl": false,
      "publisher": "Proceedings of the 2nd Joint WICOW/AIRWeb Workshop on Web Quality",
      "pages": "60-63"
    },
    "killbots": {
      "title": "Botz-4-Sale: Surviving DDos Attacks that Mimic Flash Crowds",
      "date": "",
      "authors": [
        "Srikanth Kandula",
        "Dina Katabi",
        "Matthias Jacob",
        "Arthur Burger"
      ],
      "editors": [],
      "etAl": false,
      "publisher": "",
      "href": "https://www.usenix.org/legacy/events/nsdi05/tech/kandula/kandula_html/"
    },
    "marrakehs": {
      "title": "Marrakesh Treaty to Facilitate Access to Published Works for Persons Who Are Blind, Visually Impaired or Otherwise Print Disabled",
      "authors": [],
      "editors": [],
      "etAl": false,
      "publisher": "World Intellectual Property Organization",
      "date": "27 June 2013",
      "href": "https://www.wipo.int/treaties/en/ip/marrakesh"
    },
    "newscom": {
      "title": "Spam-bot tests flunk the blind",
      "date": "2 July 2003",
      "authors": [
        "Paul Festa"
      ],
      "editors": [],
      "etAl": false,
      "publisher": "News.com",
      "href": "https://web.archive.org/web/20030707210529/http://news.com.com/2100-1032-1022814.html"
    },
    "pinguard": {
      "title": "PIN Guard",
      "date": "",
      "authors": [],
      "editors": [],
      "etAl": false,
      "publisher": "ING Direct site",
      "href": "https://secure1.ingdirect.com/tpw/popup_whatIsThis.html"
    },
    "privacy-pass": {
      "title": "Privacy Pass: Bypassing Internet Challenges Anonymously",
      "date": "2018",
      "authors": [
        "Alex Davidson",
        "Ian Goldberg",
        "Nick Sullivan",
        "George Tankersley",
        "Filippo Valsorda"
      ],
      "editors": [],
      "etAl": false,
      "publisher": "Proceedings on Privacy Enhancing technologies; 2018 (3):164-180",
      "href": "https://www.petsymposium.org/2018/files/papers/issue3/popets-2018-0026.pdf"
    },
    "pwntcha": {
      "title": "PWNtcha - CAPTCHA decoder",
      "date": "",
      "authors": [
        "Sam Hocevar"
      ],
      "editors": [],
      "etAl": false,
      "publisher": "",
      "href": "http://sam.zoy.org/pwntcha/"
    },
    "solving-captchas": {
      "authors": [
        "Elie Bursztein"
      ],
      "etAl": true,
      "editors": [],
      "title": "How good are humans at solving CAPTCHAs? A large scale evaluation",
      "publisher": "2010 IEEE symposium on security and privacy",
      "date": "2010"
    },
    "tls-tracking": {
      "title": "Exploiting TLS Client Authentication for Widespread User Tracking",
      "date": "2018",
      "authors": [
        "Lucas Foppe",
        "Jeremy Martin",
        "Travis Mayberry",
        "Eric C. Rye",
        "Lamont Brown"
      ],
      "publisher": "Proceedings on Privacy Enhancing Technologies",
      "etAl": false,
      "editors": [],
      "href": "https://www.petsymposium.org/2018/files/papers/issue4/popets-2018-0031.pdf"
    },
    "turing": {
      "title": "The Turing Test",
      "date": "2002",
      "authors": [],
      "editors": [],
      "etAl": false,
      "publisher": "he Alan Turing Internet Scrapbook",
      "href": "http://www.turing.org.uk/turing/scrapbook/test.html"
    },
    "eval-audio": {
      "authors": [
        "Bigham, J. P.",
        "Cavender, A. C."
      ],
      "date": "April 2009",
      "title": "Evaluating existing audio CAPTCHAs and an interface optimized for non-visual use",
      "publisher": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems"
    },
    "video-events": {
      "authors": [
        "Catuogno, L.",
        "Galdi, C."
      ],
      "date": "2014",
      "title": "On user authentication by means of video events recognition",
      "publisher": "Journal of Ambient Intelligence and Humanized Computing 5(6)",
      "pages": "909-918",
      "doi": "doi:10.1007/s12652-014-0248-5"
    },
    "auth-mult": {
      "authors": [
        "Cetin, C."
      ],
      "date": "2015",
      "title": "Design, Testing and Implementation of a New Authentication Method Using Multiple Devices",
      "publisher": "J. Ligatti, D. Goldgof, & Y. Liu (Eds.): ProQuest Dissertations Publishing"
    },
    "captchastar": {
      "authors": [
        "Conti, M.",
        "Guarisco, C.",
        "Spolaor, R."
      ],
      "date": "2015",
      "title": "CAPTCHaStar! A novel CAPTCHA based on interactive shape discovery"
    },
    "captcha-ld": {
      "authors": [
        "Gafni, R.",
        "Nagar, I."
      ],
      "title": "The Effect of CAPTCHA on User Experience among Users with and without Learning Disabilities"
    },
    "civil-rights-captcha": {
      "authors": [
        "Hernández-Castro, C. J.",
        "Barrero, D. F.",
        "R-Moreno, M. D."
      ],
      "date": "2016",
      "title": "Machine learning and empathy: the Civil Rights CAPTCHA",
      "publisher": "Concurrency and Computation: Practice and Experience, 28(4)",
      "pages": "1310-1323",
      "doi": "doi:10.1002/cpe.3632"
    },
    "iso-8859-1": {
      "title": "Information technology -- 8-bit single-byte coded graphic character sets -- Part 1: Latin alphabet No. 1",
      "href": "https://www.iso.org/standard/28245.html",
      "publisher": "International Organization for Standardization",
      "date": "1998"
    },
    "facecaptcha": {
      "authors": [
        "Kim, J.",
        "Kim, S.",
        "Yang, J.",
        "Ryu, J.-h.",
        "Wohn, K."
      ],
      "date": "2014",
      "title": "FaceCAPTCHA: a CAPTCHA that identifies the gender of face images unrecognized by existing gender classifiers",
      "publisher": "An International Journal, 72(2)",
      "pages": "1215-1237. ",
      "doi": "doi:10.1007/s11042-013-1422-z"
    },
    "video-captcha-security": {
      "authors": [
        "Kluever, K."
      ],
      "date": "2008",
      "title": "Evaluating the usability and security of a video CAPTCHA",
      "publisher": "R. Zanibbi, Z. Butler, & R. Canosa (Eds.): ProQuest Dissertations Publishing."
    },
    "social-classification": {
      "authors": [
        "Korayem, M."
      ],
      "date": "2015",
      "title": "Social and egocentric image classification for scientific and privacy applications",
      "publisher": "D. Crandall, J. Bollen, A. Kapadia, & P. Radivojac (Eds.): ProQuest Dissertations Publishing."
    },
    "facial-captcha-attack": {
      "authors": [
        "Li, Q."
      ],
      "date": "2015",
      "title": "A computer vision attack on the ARTiFACIAL CAPTCHA",
      "publisher": "An International Journal, 74(13)",
      "pages": "4583-4597",
      "doi": "doi:10.1007/s11042-013-1823-z"
    },
    "defeat-line-noise": {
      "authors": [
        "Nakaguro, Y.",
        "Dailey, M. N.",
        "Marukatat, S.",
        "Makhanov, S. S."
      ],
      "date": "2013",
      "title": "Defeating line-noise CAPTCHAs with multiple quadratic snakes",
      "publisher": "Computers & Security, 37",
      "pages": "91-110",
      "doi": "doi:10.1016/j.cose.2013.05.003"
    },
    "3d-captcha-security": {
      "authors": [
        "Nguyen, V. D.",
        "Chow, Y.-W.",
        "Susilo, W."
      ],
      "date": "2014",
      "title": "On the security of text-based 3D CAPTCHAs",
      "publisher": "Computers & Security, 45",
      "pages": "84-99",
      "doi": "doi:10.1016/j.cose.2014.05.004"
    },
    "recaptcha": {
      "title": "reCAPTCHA",
      "publisher": "Google",
      "href": "https://www.google.com/recaptcha/"
    },
    "recaptcha-attacks": {
      "authors": [
        "Sano, S.",
        "Otsuka, T.",
        "Itoyama, K.",
        "Okuno, H. G."
      ],
      "date": "2015",
      "title": "HMM-based Attacks on Google's ReCAPTCHA with Continuous Visual and Audio Symbols",
      "publisher": "Journal of Information Processing, 23(6)",
      "pages": "814-826",
      "doi": "doi:10.2197/ipsjjip.23.814"
    },
    "task-completion": {
      "authors": [
        "Sauer, G.",
        "Lazar, J.",
        "Hochheiser, H.",
        "Feng, J."
      ],
      "date": "2010",
      "title": "Towards a universally usable human interaction proof: evaluation of task completion strategies",
      "publisher": "ACM Transactions on Accessible Computing (TACCESS), 2(4)",
      "pages": "15"
    },
    "captcha-robustness": {
      "authors": [
        "Tangmanee, C."
      ],
      "date": "2016",
      "title": "Effects of Text Rotation, String Length, and Letter Format on Text-based CAPTCHA Robustness",
      "publisher": "Journal of Applied Security Research, 11(3)",
      "pages": "349-361",
      "doi": "doi:10.1080/19361610.2016.1178553"
    },
    "captcha-security": {
      "authors": [
        "Yan, J.",
        "El Ahmad, A. S."
      ],
      "date": "2009",
      "title": "CAPTCHA Security: A Case Study",
      "publisher": "Security & Privacy, IEEE, 7(4)",
      "doi": "doi:10.1109/MSP.2009.84"
    },
    "36-cfr-1194": {
      "title": "36 CFR Appendix C to Part 1194, Functional Performance Criteria and Technical Requirements",
      "href": "https://www.law.cornell.edu/cfr/text/36/appendix-C_to_part_1194",
      "publisher": "Legal Information Institute"
    },
    "en-301-549": {
      "title": "EN 301 549 v3.2.1: Harmonised European Standard - Accessibility requirements for ICT products and services",
      "publisher": "CEN/CENELEC/ETSI",
      "date": "2021-03",
      "href": "https://www.etsi.org/deliver/etsi_en/301500_301599/301549/03.02.01_60/en_301549v030201p.pdf"
    },
    "ETSI-ES-202-076": {
      "title": "ETSI ES 202 076 V2.1.1: Human Factors (HF); User Interfaces; Generic spoken command vocabulary for ICT devices and services",
      "publisher": "ETSI",
      "href": "https://www.etsi.org/deliver/etsi_es/202000_202099/202076/02.01.01_50/es_202076v020101m.pdf"
    },
    "mobile-auth": {
      "authors": [
        "Yeh, H. T.",
        "Chen, B. C.",
        "Wu, Y. C."
      ],
      "date": "2013",
      "title": "Mobile user authentication system in cloud environment",
      "publisher": "Security and Communication Networks, 6(9)",
      "pages": "1161-1168",
      "doi": "doi:10.1002/sec.688"
    },
    "game-captcha": {
      "authors": [
        "Yang, T.-I.",
        "Koong, C.-S.",
        "Tseng, C.-C."
      ],
      "date": "2015",
      "title": "Game-based image semantic CAPTCHA on handset devices",
      "publisher": "An International Journal, 74(14)",
      "pages": "5141-5156",
      "doi": "doi:10.1007/s11042-013-1666-7"
    },
    "marrakesh": {
      "date": "27 June 2013",
      "href": "https://www.wipo.int/treaties/en/ip/marrakesh",
      "publisher": "World Intellectual Property Organization",
      "title": "Marrakesh Treaty to Facilitate Access to Published Works for Persons Who Are Blind, Visually Impaired or Otherwise Print Disabled"
    },
    "ietf-rtc": {
      "date": "March 2015",
      "href": "https://tools.ietf.org/html/rfc7478",
      "publisher": "IETF",
      "title": "Web Real-Time Communication Use Cases and Requirements"
    },
    "ietf-relay": {
      "date": "August 2020",
      "href": "https://tools.ietf.org/html/draft-ietf-rum-rue-02.html",
      "publisher": "IETF",
      "title": "Interoperability Profile for Relay User Equipment"
    },
    "webrtc-use-cases": {
      "date": "11 December 2018",
      "href": "https://www.w3.org/TR/webrtc-nv-use-cases/",
      "publisher": "W3C",
      "title": "WebRTC Next Version Use Cases"
    },
    "rtt-sip": {
      "date": "June 2008",
      "href": "https://tools.ietf.org/html/rfc5194",
      "publisher": "IETF, Network Working Group",
      "title": "Framework for Real-Time Text over IP Using the Session Initiation Protocol (SIP)"
    },
    "EN301-549": {
      "date": "March 2021",
      "href": "https://www.etsi.org/deliver/etsi_en/301500_301599/301549/03.02.01_60/en_301549v030201p.pdf",
      "publisher": "CEN/CENELEC/ETSI",
      "title": "Accessibility requirements for ICT products and services"
    },
    "webrtc-priority": {
      "date": "12 February 2020",
      "href": "https://w3c.github.io/webrtc-priority/",
      "publisher": "W3C",
      "title": "WebRTC DSCP Control API"
    },
    "personalization": {
      "date": "27 January 2020",
      "href": "https://www.w3.org/TR/personalization-semantics-content-1.0/",
      "publisher": "W3C",
      "title": "Personalization Semantics Content Module 1.0"
    },
    "media-queries": {
      "date": "31 July 2020",
      "href": "https://www.w3.org/TR/mediaqueries-5/",
      "publisher": "W3C",
      "title": "Media Queries Level 5"
    },
    "xaur": {
      "date": "16 Sept 2020",
      "href": "https://www.w3.org/TR/xaur/",
      "publisher": "W3C",
      "title": "XR Accessibility User Requirements"
    },
    "web-adapt": {
      "authors": [
        "Matthew Tylee Atkinson",
        "Ian Hamilton",
        "Joe Humbert",
        "Kit Wessendorf"
      ],
      "date": "Dec 2018",
      "title": "W3C Workshop on Web Games Position Paper: Adaptive Accessibility",
      "href": "https://www.w3.org/2018/12/games-workshop/papers/web-games-adaptive-accessibility.html",
      "publisher": "W3C",
      "id": "web-adapt"
    },
    "inclusive-seattle": {
      "authors": [
        "W3C",
        "Pluto VR"
      ],
      "date": "Nov 2019",
      "title": "W3C Workshop on Inclusive XR Seattle",
      "href": "https://www.w3.org/2019/08/inclusive-xr-workshop/",
      "publisher": "W3C",
      "id": "inclusive-seattle"
    },
    "game-a11y": {
      "authors": [
        "Barrie Ellis",
        "Ian Hamilton",
        "Gareth Ford-Williams",
        "Lynsey Graham",
        "Dimitris Grammenos",
        "Ed Lee",
        "Jake Manion",
        "Thomas Westin"
      ],
      "date": "2019",
      "title": "Game Accessibility Guidelines",
      "href": "http://gameaccessibilityguidelines.com",
      "id": "game-a11y"
    },
    "maidenbaum-amendi": {
      "authors": [
        "Maidenbaum, S.",
        "Amedi, A"
      ],
      "date": "2015",
      "title": "Non-visual virtual interaction: Can Sensory Substitution generically increase the accessibility of Graphical virtual reality to the blind?",
      "publisher": "In Virtual and Augmented Assistive Technology (VAAT), 2015 3rd IEEE VR International Workshop on (pp. 15-17). IEEE",
      "id": "maidenbaum-amendi"
    },
    "mono-ios": {
      "authors": [
        "Apple"
      ],
      "date": "2020",
      "title": "iPhone User Guide",
      "href": "https://support.apple.com/en-gb/guide/iphone/iph3e2e2cdc/ios",
      "id": "mono-ios"
    },
    "total-conversation": {
      "authors": [
        "International Telecommunication Union (ITU)"
      ],
      "date": "2020",
      "title": "ITU-T SG 16 Work on Accessibility - Total Conversation",
      "href": "https://www.itu.int/en/ITU-T/studygroups/com16/accessibility/Pages/conversation.aspx"
    },
    "personalization-semantics": {
      "authors": [
        "Lisa Seeman",
        "Charles LaPierre",
        "Michael Cooper",
        "Roy Ran",
        "Richard Schwerdtfeger"
      ],
      "date": "2020",
      "title": "Personalization Semantics Explainer 1.0",
      "href": "https://www.w3.org/TR/personalization-semantics-1.0/",
      "publisher": "W3C",
      "id": "personalization-semantics"
    },
    "personalization-content": {
      "authors": [
        "Lisa Seeman",
        "Charles LaPierre",
        "Michael Cooper",
        "Roy Ran",
        "Richard Schwerdtfeger"
      ],
      "date": "2020",
      "title": "Personalization Semantics Content Module 1.0",
      "href": "https://www.w3.org/TR/personalization-semantics-content-1.0/",
      "publisher": "W3C",
      "id": "personalization-content"
    },
    "personalization-requirements": {
      "authors": [
        "Lisa Seeman",
        "Charles LaPierre",
        "Michael Cooper",
        "Roy Ran"
      ],
      "date": "2020",
      "title": "Requirements for Personalization Semantics",
      "href": "https://www.w3.org/TR/personalization-semantics-requirements-1.0/",
      "publisher": "W3C",
      "id": "personalization-requirements"
    },
    "supple-project": {
      "authors": [
        "Krzysztof Gajos et al"
      ],
      "date": "2010",
      "title": "SUPPLE: Automatically Generating Personalized User Interfaces",
      "href": "http://www.eecs.harvard.edu/~kgajos/research/supple/",
      "publisher": "Harvard",
      "id": "supple-project"
    },
    "spatialized-navigation": {
      "authors": [
        "Blum J.R.",
        "Bouchard M.",
        "Cooperstock J.R. "
      ],
      "date": "2012",
      "title": "What’s around Me? Spatialized Audio Augmented Reality for Blind Users with a Smartphone",
      "href": "https://link.springer.com/chapter/10.1007/978-3-642-30973-1_5",
      "publisher": "Springer",
      "id": "spatialized-navigation"
    },
    "applicability-atomic": {
      "authors": [
        "Wilco Fiers",
        "Maureen Kraft",
        "Mary Jo Mueller ",
        "Shadi Abou-Zahra"
      ],
      "date": "2019",
      "title": "Accessibility Conformance Testing (ACT) Rules Format 1.0 - W3C Recommendation, 31 October 2019",
      "href": "https://www.w3.org/TR/act-rules-format/#applicability-atomic",
      "publisher": "W3C"
    },
    "rule-types": {
      "authors": [
        "Wilco Fiers",
        "Maureen Kraft",
        "Mary Jo Mueller ",
        "Shadi Abou-Zahra"
      ],
      "date": "2019",
      "title": "Accessibility Conformance Testing (ACT) Rules Format 1.0 - W3C Recommendation, 31 October 2019",
      "href": "https://www.w3.org/TR/act-rules-format/#rule-types",
      "publisher": "W3C"
    },
    "uaag": {
      "date": "15 December 2015",
      "href": "https://www.w3.org/TR/UAAG20/",
      "publisher": "W3C",
      "title": "User Agent Accessibility Guidelines (UAAG) 2.0"
    },
    "webrtc": {
      "date": "26 January 2021",
      "href": "https://www.w3.org/TR/webrtc/",
      "publisher": "W3C",
      "title": "WebRTC 1.0: Real-Time Communication Between Browsers"
    },
    "raja-asl": {
      "date": "27 May 2021",
      "href": "https://arxiv.org/abs/2105.12928",
      "publisher": "Cornell UNiversity",
      "title": "Legibility of Videos with ASL signers",
      "id": "raja-asl"
    },
    "wfd-wasli": {
      "date": "14 March 2081",
      "href": "https://wfdeaf.org/news/resources/wfd-wasli-statement-use-signing-avatars/",
      "publisher": "World Federation of the Deaf",
      "title": "WFD and WASLI",
      "id": "wfd-wasli"
    },
    "personal-assistant-architecture": {
      "title": "Intelligent Personal Assistant Architecture and Potential for Standardization Version 1.2",
      "editors": [
        "Dirk Schnelle-Walka",
        "Deborah Dahl"
      ],
      "publisher": "Voice Interaction Community Group",
      "date": "19 July 2021",
      "href": "https://w3c.github.io/voiceinteraction/voice%20interaction%20drafts/paArchitecture-1-2.htm"
    },
    "Bragg-et-al": {
      "title": "Sign language recognition, generation, and translation: An interdisciplinary perspective",
      "authors": [
        "Danielle Bragg",
        "Oscar Koller",
        "Mary Bellard",
        "Larwan Berke",
        "Patrick Boudreault",
        "Annelies Braffort",
        "Naomi Caselli",
        "Matt Huenerfauth",
        "Hernisa Kacorri",
        "Tessa Verhoef",
        "Christian Vogler",
        "Meredith Ringel Morris"
      ],
      "publisher": "The 21st International ACM SIGACCESS Conference on Computers and Accessibility",
      "date": "October 2019"
    },
    "media-av": {
      "title": "Making Audio and Video Media Accessible",
      "publisher": "W3C web Accessibility Initiative (WAI)",
      "editors": [
        "Shawn Lawton henry"
      ],
      "date": "January 2021",
      "href": "https://www.w3.org/WAI/media/av/"
    },
    "accessible-presentations": {
      "title": "How to Make Your Presentations Accessible to All",
      "publisher": "W3C Web accessibility Initiative (WAI)",
      "editors": [
        "Shawn Lawton Henry"
      ],
      "date": "February 2021",
      "href": "https://www.w3.org/WAI/teach-advocate/accessible-presentations/"
    },
    "content-usable": {
      "title": "Making Content Usable for People with Cognitive and Learning Disabilities",
      "publisher": "W3C Web accessibility Initiative (WAI)",
      "editors": [
        "Lisa Seeman-Horwitz",
        "Rachael Bradley Montgomery",
        "Steve Lee",
        "Ruoxi Ran"
      ],
      "date": "April 2021",
      "href": "https://www.w3.org/TR/coga-usable/#user_needs"
    }
  },
  "publishISODate": "2022-04-25T00:00:00.000Z",
  "generatedSubtitle": "W3C Editor's Draft 25 April 2022"
}</script>
<link rel="stylesheet" href="https://www.w3.org/StyleSheets/TR/2021/W3C-ED"></head>
	<body class="h-entry informative"><div class="head">
    <p class="logos"><a class="logo" href="https://www.w3.org/"><img crossorigin="" alt="W3C" height="48" src="https://www.w3.org/StyleSheets/TR/2021/logos/W3C" width="72">
  </a></p>
    <h1 id="title" class="title">XR Accessibility User Requirements</h1> 
    <p id="w3c-state"><a href="https://www.w3.org/standards/types#ED">W3C Editor's Draft</a> <time class="dt-published" datetime="2022-04-25">25 April 2022</time></p>
    <details open="">
      <summary>More details about this document</summary>
      <dl>
        <dt>This version:</dt><dd>
                <a class="u-url" href="https://w3c.github.io/xaur/">https://w3c.github.io/xaur/</a>
              </dd>
        <dt>Latest published version:</dt><dd>
                <a href="https://www.w3.org/TR/xaur/">https://www.w3.org/TR/xaur/</a>
              </dd>
        <dt>Latest editor's draft:</dt><dd><a href="https://w3c.github.io/xaur/">https://w3c.github.io/xaur/</a></dd>
        <dt>History:</dt><dd>
                    <a href="https://www.w3.org/standards/history/xaur">https://www.w3.org/standards/history/xaur</a>
                  </dd><dd>
                    <a href="https://github.com/w3c/xaur/commits/">Commit history</a>
                  </dd>
        
        
        
        
        
        <dt>Editors:</dt><dd class="editor p-author h-card vcard" data-editor-id="41218">
    <a class="ed_mailto u-email email p-name" href="mailto:joconnor@w3.org">Joshue O'Connor</a> (<span class="p-org org h-org">W3C</span>)
  </dd><dd class="editor p-author h-card vcard" data-editor-id="33688">
    <a class="ed_mailto u-email email p-name" href="mailto:janina@rednote.net">Janina Sajka</a>
  </dd><dd class="editor p-author h-card vcard" data-editor-id="74028">
    <a class="ed_mailto u-email email p-name" href="mailto:jjwhite@ets.org">Jason White</a> (<span class="p-org org h-org">Educational Testing Service</span>)
  </dd><dd class="editor p-author h-card vcard">
    <a class="ed_mailto u-email email p-name" href="mailto:scott@hollier.info">Scott Hollier</a>
  </dd><dd class="editor p-author h-card vcard" data-editor-id="34017">
    <a class="ed_mailto u-email email p-name" href="mailto:cooper@w3.org">Michael Cooper</a> (<span class="p-org org h-org">W3C</span>)
  </dd>
        
        
        <dt>Feedback:</dt><dd>
        <a href="https://github.com/w3c/xaur/">GitHub w3c/xaur</a>
        (<a href="https://github.com/w3c/xaur/pulls/">pull requests</a>,
        <a href="https://github.com/w3c/xaur/issues/new/choose">new issue</a>,
        <a href="https://github.com/w3c/xaur/issues/">open issues</a>)
      </dd>
        
        
      </dl>
    </details>
    
    
    <p class="copyright">
    <a href="https://www.w3.org/Consortium/Legal/ipr-notice#Copyright">Copyright</a>
    ©
    2020-2022
    
    <a href="https://www.w3.org/"><abbr title="World Wide Web Consortium">W3C</abbr></a><sup>®</sup> (<a href="https://www.csail.mit.edu/"><abbr title="Massachusetts Institute of Technology">MIT</abbr></a>,
    <a href="https://www.ercim.eu/"><abbr title="European Research Consortium for Informatics and Mathematics">ERCIM</abbr></a>, <a href="https://www.keio.ac.jp/">Keio</a>,
    <a href="https://ev.buaa.edu.cn/">Beihang</a>). W3C
    <a href="https://www.w3.org/Consortium/Legal/ipr-notice#Legal_Disclaimer">liability</a>,
    <a href="https://www.w3.org/Consortium/Legal/ipr-notice#W3C_Trademarks">trademark</a> and
    <a rel="license" href="https://www.w3.org/Consortium/Legal/2015/copyright-software-and-document" title="W3C Software and Document Notice and License">permissive document license</a> rules apply.
  </p>
    <hr title="Separator for header">
  </div>
		<section id="abstract" class="introductory"><h2>Abstract</h2>
			
			<p>This document lists user needs and requirements for people with disabilities when using virtual reality or immersive environments, augmented or mixed reality and other related technologies (<abbr title="Virtual and Augmented Reality">XR</abbr>). It first introduces a definition of <abbr title="Virtual and Augmented Reality">XR</abbr> as used throughout the document, then briefly outlines some uses of <abbr title="Virtual and Augmented Reality">XR</abbr>. It outlines the complexity of understanding <abbr title="Virtual and Augmented Reality">XR</abbr>, introduces some technical accessibility challenges such as the need for multi-modal support, synchronization of input and output devices and customization.  It then outlines accessibility related user needs for <abbr title="Virtual and Augmented Reality">XR</abbr> and suggests subsequent requirements. This is followed by related work that may be helpful understanding the complex technical architecture and processes behind how <abbr title="Virtual and Augmented Reality">XR</abbr> environments are built and what may form the basis of a robust accessibility architecture.</p>
			
			<p>This document is most explicitly not a collection of baseline requirements. It is also important to note that some of the requirements may be implemented at a system or platform level, and some may be authoring requirements.</p>
		</section>

	<section id="sotd" class="introductory"><h2>Status of This Document</h2><p><em>This section describes the status of this
      document at the time of its publication. A list of current <abbr title="World Wide Web Consortium">W3C</abbr>
      publications and the latest revision of this technical report can be found
      in the <a href="https://www.w3.org/TR/"><abbr title="World Wide Web Consortium">W3C</abbr> technical reports index</a> at
      https://www.w3.org/TR/.</em></p>

			<p>To comment on this draft <a href="https://github.com/w3c/apa/issues/new">file an issue in the <abbr title="World Wide Web Consortium">W3C</abbr> APA GitHub repository</a>. If this is not feasible, send email to <a href="mailto:public-apa@w3.org">public-apa@w3.org</a> (<a href="https://lists.w3.org/Archives/Public/public-apa/">archives</a>). In-progress updates to the document may be viewed in the <a href="https://w3c.github.io/apa/xaur/">publicly visible editors' draft</a>.</p>
		<p>
    This document was published by the <a href="https://www.w3.org/groups/wg/apa">Accessible Platform Architectures Working Group</a> as
    an Editor's Draft. 
  </p><p>Publication as an Editor's Draft does not
  imply endorsement by <abbr title="World Wide Web Consortium">W3C</abbr> and its Members. </p><p>
    This is a draft document and may be updated, replaced or obsoleted by other
    documents at any time. It is inappropriate to cite this document as other
    than work in progress.
    
  </p><p>
    
        This document was produced by a group
        operating under the
        <a href="https://www.w3.org/Consortium/Patent-Policy/"><abbr title="World Wide Web Consortium">W3C</abbr> Patent
          Policy</a>.
      
    
                <abbr title="World Wide Web Consortium">W3C</abbr> maintains a
                <a rel="disclosure" href="https://www.w3.org/groups/wg/apa/ipr">public list of any patent disclosures</a>
          made in connection with the deliverables of
          the group; that page also includes
          instructions for disclosing a patent. An individual who has actual
          knowledge of a patent which the individual believes contains
          <a href="https://www.w3.org/Consortium/Patent-Policy/#def-essential">Essential Claim(s)</a>
          must disclose the information in accordance with
          <a href="https://www.w3.org/Consortium/Patent-Policy/#sec-Disclosure">section 6 of the <abbr title="World Wide Web Consortium">W3C</abbr> Patent Policy</a>.
        
  </p><p>
                  This document is governed by the
                  <a id="w3c_process_revision" href="https://www.w3.org/2021/Process-20211102/">2 November 2021 <abbr title="World Wide Web Consortium">W3C</abbr> Process Document</a>.
                </p></section><nav id="toc"><h2 class="introductory" id="table-of-contents">Table of Contents</h2><ol class="toc"><li class="tocline"><a class="tocxref" href="#abstract">Abstract</a></li><li class="tocline"><a class="tocxref" href="#sotd">Status of This Document</a></li><li class="tocline"><a class="tocxref" href="#introduction"><bdi class="secno">1. </bdi>Introduction</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#what-does-the-term-xr-mean"><bdi class="secno">1.1 </bdi>What does the term '<abbr title="Virtual and Augmented Reality">XR</abbr>' mean?</a></li><li class="tocline"><a class="tocxref" href="#definitions-of-virtual-reality-and-immersive-environments"><bdi class="secno">1.2 </bdi>Definitions of virtual reality and immersive environments</a></li><li class="tocline"><a class="tocxref" href="#definitions-of-augmented-and-mixed-reality"><bdi class="secno">1.3 </bdi>Definitions of augmented and mixed reality</a></li></ol></li><li class="tocline"><a class="tocxref" href="#what-is-xr-used-for"><bdi class="secno">2. </bdi>What is <abbr title="Virtual and Augmented Reality">XR</abbr> used for?</a></li><li class="tocline"><a class="tocxref" href="#understanding-xr-and-accessibility-challenges"><bdi class="secno">3. </bdi>Understanding <abbr title="Virtual and Augmented Reality">XR</abbr> and Accessibility Challenges</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#immersive-environment-challenges"><bdi class="secno">3.1 </bdi>Immersive Environment challenges</a></li><li class="tocline"><a class="tocxref" href="#xr-and-supporting-multimodality"><bdi class="secno">3.2 </bdi><abbr title="Virtual and Augmented Reality">XR</abbr> and supporting multimodality</a></li><li class="tocline"><a class="tocxref" href="#various-input-modalities"><bdi class="secno">3.3 </bdi>Various input modalities </a></li><li class="tocline"><a class="tocxref" href="#various-output-modalities"><bdi class="secno">3.4 </bdi>Various output modalities </a></li><li class="tocline"><a class="tocxref" href="#xr-controller-challenges"><bdi class="secno">3.5 </bdi><abbr title="Virtual and Augmented Reality">XR</abbr> controller challenges</a></li><li class="tocline"><a class="tocxref" href="#customization-of-control-inputs"><bdi class="secno">3.6 </bdi>Customization of control inputs</a></li><li class="tocline"><a class="tocxref" href="#using-multiple-diverse-inputs-simultaneously"><bdi class="secno">3.7 </bdi> Using multiple diverse inputs simultaneously </a></li><li class="tocline"><a class="tocxref" href="#consistent-tracking-with-multiple-inputs"><bdi class="secno">3.8 </bdi>Consistent tracking with multiple inputs </a></li><li class="tocline"><a class="tocxref" href="#usability-and-affordances-in-xr"><bdi class="secno">3.9 </bdi>Usability and affordances in <abbr title="Virtual and Augmented Reality">XR</abbr></a></li></ol></li><li class="tocline"><a class="tocxref" href="#xr-user-needs-and-requirements"><bdi class="secno">4. </bdi><abbr title="Virtual and Augmented Reality">XR</abbr> User Needs and Requirements</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#immersive-semantics-and-customization"><bdi class="secno">4.1 </bdi>Immersive semantics and customization</a></li><li class="tocline"><a class="tocxref" href="#motion-agnostic-interactions"><bdi class="secno">4.2 </bdi>Motion agnostic interactions</a></li><li class="tocline"><a class="tocxref" href="#immersive-personalization"><bdi class="secno">4.3 </bdi>Immersive personalization</a></li><li class="tocline"><a class="tocxref" href="#interaction-and-target-customization"><bdi class="secno">4.4 </bdi>Interaction and target customization</a></li><li class="tocline"><a class="tocxref" href="#voice-commands"><bdi class="secno">4.5 </bdi>Voice commands</a></li><li class="tocline"><a class="tocxref" href="#color-changes"><bdi class="secno">4.6 </bdi>Color changes</a></li><li class="tocline"><a class="tocxref" href="#magnification-context-and-resetting"><bdi class="secno">4.7 </bdi>Magnification context and resetting</a></li><li class="tocline"><a class="tocxref" href="#critical-messaging-and-alerts"><bdi class="secno">4.8 </bdi>Critical messaging and alerts</a></li><li class="tocline"><a class="tocxref" href="#gestural-interfaces-and-interactions"><bdi class="secno">4.9 </bdi>Gestural interfaces and interactions</a></li><li class="tocline"><a class="tocxref" href="#signing-videos-and-text-description-transformation"><bdi class="secno">4.10 </bdi>Signing videos and text description transformation</a></li><li class="tocline"><a class="tocxref" href="#safe-harbour-controls"><bdi class="secno">4.11 </bdi>Safe harbour controls</a></li><li class="tocline"><a class="tocxref" href="#immersive-time-limits"><bdi class="secno">4.12 </bdi>Immersive time limits</a></li><li class="tocline"><a class="tocxref" href="#orientation-and-navigation"><bdi class="secno">4.13 </bdi>Orientation and navigation</a></li><li class="tocline"><a class="tocxref" href="#second-screen-devices"><bdi class="secno">4.14 </bdi>Second screen devices</a></li><li class="tocline"><a class="tocxref" href="#interaction-speed"><bdi class="secno">4.15 </bdi>Interaction speed</a></li><li class="tocline"><a class="tocxref" href="#avoiding-sickness-triggers"><bdi class="secno">4.16 </bdi>Avoiding sickness triggers</a></li><li class="tocline"><a class="tocxref" href="#spatial-audio-tracks-and-alternatives"><bdi class="secno">4.17 </bdi>Spatial audio tracks and alternatives</a></li><li class="tocline"><a class="tocxref" href="#spatial-orientation-mono-audio-option"><bdi class="secno">4.18 </bdi>Spatial orientation: Mono audio option</a></li><li class="tocline"><a class="tocxref" href="#captioning-subtitling-and-text-support-and-customization"><bdi class="secno">4.19 </bdi>Captioning, Subtitling and Text: Support and customization</a></li></ol></li><li class="tocline"><a class="tocxref" href="#related-documents"><bdi class="secno">5. </bdi>Related Documents</a></li><li class="tocline"><a class="tocxref" href="#c-change-log"><bdi class="secno">A. </bdi>Change Log<span class="formerLink" aria-label="§"></span></a></li><li class="tocline"><a class="tocxref" href="#acknowledgements"><bdi class="secno">B. </bdi>Acknowledgements</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#participants-of-the-apa-working-group-active-in-the-development-of-this-document"><bdi class="secno">B.1 </bdi>Participants of the APA working group active in the development of this document</a></li><li class="tocline"><a class="tocxref" href="#previously-active-participants-commenters-and-other-contributors"><bdi class="secno">B.2 </bdi>Previously active participants, commenters, and other contributors</a></li><li class="tocline"><a class="tocxref" href="#enabling-funders"><bdi class="secno">B.3 </bdi>Enabling Funders</a></li></ol></li><li class="tocline"><a class="tocxref" href="#references"><bdi class="secno">C. </bdi>References</a><ol class="toc"><li class="tocline"><a class="tocxref" href="#informative-references"><bdi class="secno">C.1 </bdi>Informative references</a></li></ol></li></ol></nav>
		<section id="introduction"><div class="header-wrapper"><h2 id="x1-introduction"><bdi class="secno">1. </bdi>Introduction</h2><a class="self-link" href="#introduction" aria-label="Permalink for Section 1."></a></div>
			

<p><abbr title="Virtual and Augmented Reality">XR</abbr> is an acronym used to refer to the spectrum of hardware, applications, and techniques used for virtual reality or immersive environments, augmented or mixed reality and other related technologies. This document is developed as part of a discovery into accessibility related user needs and requirements for <abbr title="Virtual and Augmented Reality">XR</abbr>. This document does not represent a formal working group position, nor does it currently represent a set of technical requirements that a developer or designer need strictly follow. It aims to outline the diversity of some current accessibility related user needs in <abbr title="Virtual and Augmented Reality">XR</abbr> and what potential requirements to meet those needs may be.</p>


<section id="what-does-the-term-xr-mean"><div class="header-wrapper"><h3 id="x1-1-what-does-the-term-xr-mean"><bdi class="secno">1.1 </bdi>What does the term '<abbr title="Virtual and Augmented Reality">XR</abbr>' mean?</h3><a class="self-link" href="#what-does-the-term-xr-mean" aria-label="Permalink for Section 1.1"></a></div>

<p>As with the <a href="https://immersive-web.github.io/webxr/">WebXR API spec</a> and as indicated in the related <a href="https://github.com/immersive-web/webxr/blob/master/explainer.md#what-is-webxr">WebXR explainer</a>, this document uses the acronym <abbr title="Virtual and Augmented Reality">XR</abbr> to refer to the spectrum of hardware, applications, and techniques used for virtual reality or immersive environments, augmented or mixed reality and other related technologies. Examples include, but are not limited to:</p>

<ul>

<li>Immersive or augmented environments used for education, gaming, multimedia, 360° content and other applications.</li>
<li>Head mounted displays, whether they are opaque, transparent, or utilise video passthrough.</li>
<li>Mobile devices with positional tracking.</li>
<li>Fixed displays with head tracking capabilities.</li>
</ul>

<p>The important commonality between them being that they all offer some degree of spatial tracking with which to simulate a view of virtual content as well as navigation and interaction with the objects within these environments.</p>

<p>Terms like "<abbr title="Virtual and Augmented Reality">XR</abbr> Device", "<abbr title="Virtual and Augmented Reality">XR</abbr> Application", etc. are generally understood to apply to any of the above. Portions of this document that only apply to a subset of these devices will be indicated as appropriate.</p>
</section>

	<section id="definitions-of-virtual-reality-and-immersive-environments"><div class="header-wrapper"><h3 id="x1-2-definitions-of-virtual-reality-and-immersive-environments"><bdi class="secno">1.2 </bdi>Definitions of virtual reality and immersive environments</h3><a class="self-link" href="#definitions-of-virtual-reality-and-immersive-environments" aria-label="Permalink for Section 1.2"></a></div>


<p>Virtual reality and immersive environment definitions vary but converge on the notion of immersive computer-mediated experiences. They involve interaction with objects, people and environments using a range of controls. These experiences are often multi-sensory and may be used for educational, therapeutic or entertainment purposes.</p>
	</section>

		<section id="definitions-of-augmented-and-mixed-reality"><div class="header-wrapper"><h3 id="x1-3-definitions-of-augmented-and-mixed-reality"><bdi class="secno">1.3 </bdi>Definitions of augmented and mixed reality</h3><a class="self-link" href="#definitions-of-augmented-and-mixed-reality" aria-label="Permalink for Section 1.3"></a></div>


<p>Augmented and mixed reality definitions vary but converge on the notion of computer-mediated interactions involving overlays on the real world.  These may be informational, or interactive depending on the application.</p>
	</section>	

</section>


<section id="what-is-xr-used-for"><div class="header-wrapper"><h2 id="x2-what-is-xr-used-for"><bdi class="secno">2. </bdi>What is <abbr title="Virtual and Augmented Reality">XR</abbr> used for?</h2><a class="self-link" href="#what-is-xr-used-for" aria-label="Permalink for Section 2."></a></div>


<p><abbr title="Virtual and Augmented Reality">XR</abbr> has a range of purposes from work, education, gaming, multimedia and communication. It is evolving at a fast rate and while not yet mainstream, this will change as computing power increases, hardware becomes cheaper and the quality of the user experience improves. <abbr title="Virtual and Augmented Reality">XR</abbr> will be more commonly used for the performance of work tasks, for therapeutic uses, education and for entertainment.</p>
		</section>



	<section id="understanding-xr-and-accessibility-challenges"><div class="header-wrapper"><h2 id="x3-understanding-xr-and-accessibility-challenges"><bdi class="secno">3. </bdi>Understanding <abbr title="Virtual and Augmented Reality">XR</abbr> and Accessibility Challenges</h2><a class="self-link" href="#understanding-xr-and-accessibility-challenges" aria-label="Permalink for Section 3."></a></div>
		
		


<p>Understanding <abbr title="Virtual and Augmented Reality">XR</abbr> itself presents various challenges that are technical. They include issues with a range of hardware, software and authoring tools. To make accessible <abbr title="Virtual and Augmented Reality">XR</abbr> experiences there is a need to understand interaction design principles, accessibility semantics and assistive technologies. However, these all represent 'basic' complexities that are in themselves substantial. To add to this, for many designers and authors they may neither know nor have access to people with disabilities for usability testing. Neither may they have a practical way of understanding accessibility related user needs that they can build a solid set of requirements from. In short, they just may not understand what user needs they are trying to meet.</p>

<p>Some of the issues in <abbr title="Virtual and Augmented Reality">XR</abbr>, for example in gaming, for people with disabilities include:</p>

<ul>
<li><strong>Over emphasis on motion controls</strong>. There are many motion controllers that emphasise using your body to control the experience. Some games with <abbr title="Virtual and Augmented Reality">XR</abbr> components may lock out traditional control methods when a VR headset is being used, and the user should always be able to use a range of input mechanisms. <br>

<div class="note" role="note" id="issue-container-generatedID"><div role="heading" class="note-title marker" id="h-note" aria-level="3"><span>Note</span></div><div class=""><abbr title="Three Degrees of Freedom">3DOF</abbr> and <abbr title="Six Degrees of Freedom">6DOF</abbr> may have their own specific mobility issues, for example <abbr title="Three Degrees of Freedom">3DOF</abbr> may have implications for people who have motor impairments that affect the use of one or both arms.  <abbr title="Six Degrees of Freedom">6DOF</abbr> may have implications for people who are quadriplegic and for people that use a wheelchair or mobility aid for navigation where there is a need to move directionally in physical space or a higher emphasis on the lower extremity for movement.</div></div>
</li>
<li><strong>VR Headsets need the user to be a physical position to play</strong>. The user should not have to be in a particular physical position such as standing or sitting to play a game or perform some action. Or there should be ability to remap these 'physical positions' to other controls (such as using <a href="https://www.walkinvrdriver.com">WalkinVRDriver</a>).</li>
<li><strong>Games and hardware being locked to certain manufacturers</strong>. Consoles should allow full button remapping on standard game controllers - to different types of assistive technologies such as switches. These remapping preferences should be mobile, and transportable across a range of hardware devices and software.</li>
<li><strong>Gamification of VR forces game dynamics on the user</strong>. Some users may wish to just explore an immersive environment without the 'game'.</li>
<li><strong>Audio design lacks spatial accuracy</strong>. Sound design needs particular attention and can be critical for a good user experience for people with disabilities. The auditory experience of a game or other immersive environment may 'be' the experience [<cite><a class="bibref" data-link-type="biblio" href="#bib-able-gamers" title="Thought On Accessibility and VR">able-gamers</a></cite>].</li>

</ul>

<p>There are a range of disabilities that will need to be considered in making <abbr title="Virtual and Augmented Reality">XR</abbr> accessible. It is beyond the scope of this document to describe them all in detail. General categories or types of disabilities are:</p>

<ul>
<li>Auditory disabilities</li>
<li>Cognitive disabilities</li>
<li>Neurological disabilities</li>
<li>Physical disabilities</li>
<li>Speech disabilities</li>
<li>Visual disabilities</li>
</ul>


<p>A person may have one of these disabilities or a combination of several. User needs are presented here that may relate to several of these disabilities with a range of requirements that should be met by the author or the platform. For <abbr title="Virtual and Augmented Reality">XR</abbr> designers and authors understanding these needs is crucial when making <abbr title="Virtual and Augmented Reality">XR</abbr> environments accessible.</p> 

<p>Some things designers and authors need to be aware of:</p>

<ul>
<li>Understanding specific diverse user needs and how they relate to <abbr title="Virtual and Augmented Reality">XR</abbr>.</li>
<li>Successfully identifying modality needs that are not obvious - but still need to be supported.</li>
<li>Suitable authoring tools that support accessibility requirements in <abbr title="Virtual and Augmented Reality">XR</abbr>.</li>
<li>Using languages, platforms and engines that support accessibility semantics.</li>
<li>Providing accessible alternatives for content and interaction.</li>
<li>The provision of specific commands within the VR environment (e.g., to go directly to a specified location or to follow another user) which assist with navigation to support different modalities.</li>
<li>The use of virtual assistive technologies (e.g., white cane via a haptic device) to provide non-visual feedback. The research identified that if the same audio cues associated with a real-world infrared white cane were used in immersive environment, users were able to effectively centre themselves in the middle of pathways and walk successfully through virtual doorways based on the same audio feedback as used in the equivalent real-world device [<cite><a class="bibref" data-link-type="biblio" href="#bib-maidenbaum-amendi" title="Non-visual virtual interaction: Can Sensory Substitution generically increase the accessibility of Graphical virtual reality to the blind?">maidenbaum-amendi</a></cite>]</li>
</ul>


	<section id="immersive-environment-challenges"><div class="header-wrapper"><h3 id="x3-1-immersive-environment-challenges"><bdi class="secno">3.1 </bdi>Immersive Environment challenges</h3><a class="self-link" href="#immersive-environment-challenges" aria-label="Permalink for Section 3.1"></a></div>


<p>Some of the challenges within immersive environments (and gaming) accessibility include the use of extremely complex input devices, control schemes that require a high degree of precision, timing and simultaneous action; ability to distinguish subtle differences in busy visual and audio information, having to juggle multiple complex goals and objectives [<cite><a class="bibref" data-link-type="biblio" href="#bib-web-adapt" title="W3C Workshop on Web Games Position Paper: Adaptive Accessibility">web-adapt</a></cite>].</p>

	<p>There are also currently very useful accessibility guidelines available that are specific to gaming [<cite><a class="bibref" data-link-type="biblio" href="#bib-game-a11y" title="Game Accessibility Guidelines">game-a11y</a></cite>].</p>
</section>

	<section id="xr-and-supporting-multimodality"><div class="header-wrapper"><h3 id="x3-2-xr-and-supporting-multimodality"><bdi class="secno">3.2 </bdi><abbr title="Virtual and Augmented Reality">XR</abbr> and supporting multimodality</h3><a class="self-link" href="#xr-and-supporting-multimodality" aria-label="Permalink for Section 3.2"></a></div>

<p>Modality relates to modes of sense perception such as sight, hearing, touch and so on. Accessibility can be thought of as supporting multi-modal requirements and the transformation of content or aspects of a user interface from one mode to another that will support various user needs.</p> 

<p>Considering various modality requirements in the foundation of <abbr title="Virtual and Augmented Reality">XR</abbr> means these platforms will be better able to support accessibility related user needs. There will be many modality aspects for the developer and/or content author to consider.</p> 


<p><abbr title="Virtual and Augmented Reality">XR</abbr> authors and content designers will also need access to tools that support the multi-modal requirements listed below. </p>

<p>The following inputs and outputs can be considered modalities that should be supported in <abbr title="Virtual and Augmented Reality">XR</abbr> environments.</p>

</section>

<section id="various-input-modalities"><div class="header-wrapper"><h3 id="x3-3-various-input-modalities"><bdi class="secno">3.3 </bdi>Various input modalities </h3><a class="self-link" href="#various-input-modalities" aria-label="Permalink for Section 3.3"></a></div>
 

<p>The following are example of some of the diverse input methods used by people with disabilities. In many real-world applications these input methods may be combined.</p>

<ul>
<li><strong>Speech</strong> - this is where a user's voice is the main input. Using a range of speech commands, a user should be able to navigate in an <abbr title="Virtual and Augmented Reality">XR</abbr> environment, interact with the objects in that environment using their voice alone.</li>
<li><strong>Keyboard </strong> - this is where the keyboard alone is the user's main input. A user should be able to navigate in an <abbr title="Virtual and Augmented Reality">XR</abbr> environment, interact with the objects in that environment using the keyboard alone.</li>
<li><strong>Switch </strong> this is where a since button Switch alone is the user's main input. A user should be able to navigate in an <abbr title="Virtual and Augmented Reality">XR</abbr> environment, interact with the objects in that environment using a Switch alone. This switch may be used in conjunction with an assistive technology scanning application within the <abbr title="Virtual and Augmented Reality">XR</abbr> environment that allows them to select directions for navigation, macros for communication and interaction.</li>
<li><strong>Gesture </strong> - this is where gesture-based controllers are the main input and can be used to navigate in an <abbr title="Virtual and Augmented Reality">XR</abbr> environment, interact with the objects in that environment make selections using their voice alone.</li>
<li><strong>Eye Tracking</strong> - this is where eye tracking applications is the main input. Using a range of commands, a user should be able to navigate in an <abbr title="Virtual and Augmented Reality">XR</abbr> environment, interact with the objects in that environment using these eye tracking applications.</li>
</ul>

</section>

<section id="various-output-modalities"><div class="header-wrapper"><h3 id="x3-4-various-output-modalities"><bdi class="secno">3.4 </bdi>Various output modalities </h3><a class="self-link" href="#various-output-modalities" aria-label="Permalink for Section 3.4"></a></div>


<p>The following are a list of outputs that can be available to a user to help them understand, interact with and 'sense' feedback from an <abbr title="Virtual and Augmented Reality">XR</abbr> application. Some of these are in common use on the Web and other exploratory (such as Olfactory and Gustatory.)</p>

<ul>

<li>Tactile - this is using the sense of touch, or commonly referred to as haptics.</li>
<li>Visual - this is using the sense of sight, such as 2D and 3D graphics.</li>
<li>Auditory - this is using the sense of sound, such as rich spatial audio, surround sound.</li>
<li>Olfactory - this is the sense of smell.</li>
<li>Gustatory  - this is the sense of taste.</li>
</ul>


</section>

<section id="xr-controller-challenges"><div class="header-wrapper"><h3 id="x3-5-xr-controller-challenges"><bdi class="secno">3.5 </bdi><abbr title="Virtual and Augmented Reality">XR</abbr> controller challenges</h3><a class="self-link" href="#xr-controller-challenges" aria-label="Permalink for Section 3.5"></a></div>


<p>As mentioned, there are a range of input devices that may be used. Supporting these controllers requires an understanding of what they are and how they work.
There are a variety of alternative gaming controls that may be very useful in <abbr title="Virtual and Augmented Reality">XR</abbr> environments and applications. For example, the <a href="https://www.xbox.com/en-US/xbox-one/accessories/controllers/xbox-adaptive-controller ">Xbox Adaptive Controller</a>.</p>

<p>While <abbr title="Virtual and Augmented Reality">XR</abbr> is the experience, the controller plays a critical part in overcoming some complexity as well as mediating issues that may relate to other challenges around usability and helping the user understand sensory substitution devices. </p>

<p>Controllers such as the Xbox Adaptive Controller and other switch type inputs allow the user to remap keyboard inputs to control or interact with virtual environments. These powerful customizations allow the user to "do that thing that is difficult" for them with ease.  In conjunction with this controller, for example, users with limited mobility they can also simulate actions in the <abbr title="Virtual and Augmented Reality">XR</abbr> environment that they would not be able to physically perform.  <a href="https://www.walkinvrdriver.com">WalkinVRDriver</a> is a good example of this where motion range, position and orientation can be set to the user's ability.</p>

</section>

	<section id="customization-of-control-inputs"><div class="header-wrapper"><h3 id="x3-6-customization-of-control-inputs"><bdi class="secno">3.6 </bdi>Customization of control inputs</h3><a class="self-link" href="#customization-of-control-inputs" aria-label="Permalink for Section 3.6"></a></div>

<p>Give the user the ability to modify their input preference or use a variety of input devices. The remapping of keys used to control movement or interaction in virtual environments is not currently required by WCAG. It is nevertheless noted in the literature as desirable.</p>

</section>

	<section id="using-multiple-diverse-inputs-simultaneously"><div class="header-wrapper"><h3 id="x3-7-using-multiple-diverse-inputs-simultaneously"><bdi class="secno">3.7 </bdi> Using multiple diverse inputs simultaneously </h3><a class="self-link" href="#using-multiple-diverse-inputs-simultaneously" aria-label="Permalink for Section 3.7"></a></div>

<p>A user with a disability may have several input devices or different assistive technologies. A user may switch 'mode' of interaction, or the tools used without degrading the user experience where they lose focus on a task and cannot return to it, or make unwanted input.</p>

<p>Complexity needs to be managed and co-ordinated between different kinds of assistive technology in immersive environments. There is a platform level requirement to support multiple assistive technologies in a cohesive manner. This would allow combinations to be used in a co-ordinated way e.g. where the users day-to-day AT, can be used with other AT that may be embedded in the environment already for example.</p>

<div class="note" role="note" id="issue-container-generatedID-0"><div role="heading" class="note-title marker" id="h-note-0" aria-level="4"><span>Note</span></div><p class="">The REQ 5b: Voice activation also indicates potential issues with pairing multiple devices via Bluetooth.</p></div>

</section>

	<section id="consistent-tracking-with-multiple-inputs"><div class="header-wrapper"><h3 id="x3-8-consistent-tracking-with-multiple-inputs"><bdi class="secno">3.8 </bdi>Consistent tracking with multiple inputs </h3><a class="self-link" href="#consistent-tracking-with-multiple-inputs" aria-label="Permalink for Section 3.8"></a></div>

<p>There may be tracking issues when switching input devices. A tracking issue is where the user may lose their focus or it can be modified in unpredictable or unwanted ways, this can cause loss of focus and potentially push the user to make unwanted inputs or choices.</p>

<p>Outputs sent to multiple devices will need to be synchronised.</p>

	</section>


<section id="usability-and-affordances-in-xr"><div class="header-wrapper"><h3 id="x3-9-usability-and-affordances-in-xr"><bdi class="secno">3.9 </bdi>Usability and affordances in <abbr title="Virtual and Augmented Reality">XR</abbr></h3><a class="self-link" href="#usability-and-affordances-in-xr" aria-label="Permalink for Section 3.9"></a></div>

<p>An <abbr title="Virtual and Augmented Reality">XR</abbr> application should have a high level of usability for someone with a disability using assistive technology. Therefore, communicating affordances successfully is critical and needs to be done in a way that supports multiple modalities. Some related questions are:</p>
  
<ul>
<li>How can affordances be successfully translated from one modality to another?</li>
<li>Can affordances be mediated or transformed, as needed by the users own modality preferences?</li>
<li>Should affordances change depending on context of use? What interactions are allowed or not allowed? </li>
<li>How can we ensure what happens in one modality, is reflected in another? So various modalities are not out of sync e.g. synchronization of captions between real time text transcriptions and other alternatives such as symbols or AAC?</li>
</ul>

<div class="note" role="note" id="issue-container-generatedID-1"><div role="heading" class="note-title marker" id="h-note-1" aria-level="4"><span>Note</span></div><p class="">Regarding the discoverability of accessibility features in <abbr title="Virtual and Augmented Reality">XR</abbr>. It is important for designers of accessible <abbr title="Virtual and Augmented Reality">XR</abbr> to understand how to categorize various accessibility features and understand where to place them, in a menu for example. An accessibility related accommodation may have multiple contexts of use that may not be obvious. For example, the suggested use of "mono" in User Need 19 is not just an accessibility feature under a hearing-impaired category, as it is also useful for users with spatial orientation impairments or cognitive and learning disabilities. Care should be taken to ensure these features are categorized in menus correctly and discoverable in multiple contexts.</p></div>

	</section>
</section>

<section id="xr-user-needs-and-requirements"><div class="header-wrapper"><h2 id="x4-xr-user-needs-and-requirements"><bdi class="secno">4. </bdi><abbr title="Virtual and Augmented Reality">XR</abbr> User Needs and Requirements</h2><a class="self-link" href="#xr-user-needs-and-requirements" aria-label="Permalink for Section 4."></a></div>


<p>This document outlines various accessibility related user needs for <abbr title="Virtual and Augmented Reality">XR</abbr>. These user needs should drive accessibility requirements for <abbr title="Virtual and Augmented Reality">XR</abbr> and its related architecture. These come from people with disabilities who use assistive technologies and wish to see the features described available within <abbr title="Virtual and Augmented Reality">XR</abbr> enabled applications.</p>

<p>User needs and requirements are often dependent on context of use. The following outline some accessibility user needs and requirements that may be applicable in immersive environments, augmented reality and 360° applications.</p>

<p>These following are neither exhaustive, nor definitive but are presented to help orientate the reader towards understanding some broad user needs and how to meet them. </p>


	<section id="immersive-semantics-and-customization"><div class="header-wrapper"><h3 id="x4-1-immersive-semantics-and-customization"><bdi class="secno">4.1 </bdi>Immersive semantics and customization</h3><a class="self-link" href="#immersive-semantics-and-customization" aria-label="Permalink for Section 4.1"></a></div>


<ul>

<li><strong>User Need 1:</strong> A user of assistive technology wants to navigate, identify locations, objects and interact within an immersive environment.</li>

<li><strong>REQ 1a:</strong> Navigation mechanisms must be intuitive with robust affordances. Navigation, location and object descriptions must be accurate and identified in a way that is understood by assistive technology. </li>


<li><strong>REQ 1b:</strong> Controls need to support alternative mapping, rearranging of position, resizing and sensitivity adjustment.</li>
<li><strong>REQ 1c:</strong> Objects that are important within any given context of time and place can be identified in a suitable modality.</li>
<li><strong>REQ 1d:</strong>  Allow the user to filter or sort objects and content.</li>
<li><strong>REQ 1e:</strong>  Allow the user to query objects and content for more details.</li>


</ul>
		<div class="note" role="note" id="issue-container-generatedID-2"><div role="heading" class="note-title marker" id="h-note-2" aria-level="4"><span>Note</span></div><p class="">In an spatialized augmented reality environment a blind user may find a combination of text to speech and sonic symbols helpful.  By using a combination of text to speech and sonic symbolism a blind user can do a self-guided tour of a given area using their smartphone. [<cite><a class="bibref" data-link-type="biblio" href="#bib-spatialized-navigation" title="What’s around Me? Spatialized Audio Augmented Reality for Blind Users with a Smartphone">spatialized-navigation</a></cite>]</p></div>
		
		
	</section>
	<section id="motion-agnostic-interactions"><div class="header-wrapper"><h3 id="x4-2-motion-agnostic-interactions"><bdi class="secno">4.2 </bdi>Motion agnostic interactions</h3><a class="self-link" href="#motion-agnostic-interactions" aria-label="Permalink for Section 4.2"></a></div>

	
<ul>

<li><strong>User Need 2:</strong> A person with a physical disability may want to interact with items in an immersive environment&nbsp;in a way that doesn't require particular bodily movement to perform any given action.</li>

<li><strong>REQ 2a:</strong> Allow the user performing an action in the environment, in a device independent way, without having to do so physically.</li>

<li><strong>REQ 2b:</strong> Ensure that all areas of the user interface can be accessed using the same input method.</li>
<li><strong>REQ 2c:</strong> Allow multiple input methods to be used at the same time.</li>

</ul>
<div class="note" role="note" id="issue-container-generatedID-3"><div role="heading" class="note-title marker" id="h-note-3" aria-level="4"><span>Note</span></div><p class="">There are accessibility issues specific to augmented reality. For example, the user may be expected to scan the environment, or scan physical objects, to determine the placement of virtual objects. The user may need to mark a location or an area in space so that the AR application can generate appropriate virtual objects. The user should be able to perform these actions in a motion agnostic way.</p></div>



</section>

<section id="immersive-personalization"><div class="header-wrapper"><h3 id="x4-3-immersive-personalization"><bdi class="secno">4.3 </bdi>Immersive personalization</h3><a class="self-link" href="#immersive-personalization" aria-label="Permalink for Section 4.3"></a></div>



<ul>

<li><strong>User Need 3:</strong> Users with cognitive and learning disabilities may need to personalize the immersive experience in various ways.</li>

<li><strong>REQ 3a:</strong> Support Symbol sets so they can be used to communicate and layered over objects and items to convey affordances or other needed information in way that can be understood according to user preference.</li>


<li><strong>REQ 3b:</strong> Allow the user to turn off or 'mute' non-critical environmental content such as animations, visual or audio content, or non-critical messaging.</li>

</ul>

<div class="note" role="note" id="issue-container-generatedID-4"><div role="heading" class="note-title marker" id="h-note-4" aria-level="4"><span>Note</span></div><aside class="">

<p>Personalization involves tailoring aspects of the user experience to meet the needs and preferences of the individual user. <abbr title="World Wide Web Consortium">W3C</abbr> are working on various modules for web content that aim to support personalization and are exploring areas such as:</p>

<ul>
<li>Expanding the accessibility information that may be supplied by the author. [<cite><a class="bibref" data-link-type="biblio" href="#bib-personalization-semantics" title="Personalization Semantics Explainer 1.0">personalization-semantics</a></cite>]</li>
<li>Facilitating preference driven individual personalization. [<cite><a class="bibref" data-link-type="biblio" href="#bib-personalization-content" title="Personalization Semantics Content Module 1.0">personalization-content</a></cite>] </li>
<li>Enabling the author to specify key semantics needed to support users with cognitive impairments. [<cite><a class="bibref" data-link-type="biblio" href="#bib-personalization-requirements" title="Requirements for Personalization Semantics">personalization-requirements</a></cite>]</li>
</ul>
</aside></div>


</section>

<section id="interaction-and-target-customization"><div class="header-wrapper"><h3 id="x4-4-interaction-and-target-customization"><bdi class="secno">4.4 </bdi>Interaction and target customization</h3><a class="self-link" href="#interaction-and-target-customization" aria-label="Permalink for Section 4.4"></a></div>



<ul>

<li><strong>User Need 4:</strong> A user with limited mobility, or users with tunnel or peripheral vision may need a larger 'Target size'&nbsp;for a button or other controls.</li>

<li><strong>REQ 4a:</strong> Ensure fine motion control is not needed to activate an input.</li>
<li><strong>REQ 4b:</strong> Ensure hit targets are large enough with suitable spacing around them.</li>
<li><strong>REQ 4c:</strong> Ensure multiple actions or gestures are not required at the same time to perform any action.</li>
<li><strong>REQ 4d:</strong> Support 'Sticky Keys' requirements such as serialization for various inputs when the user needs to press multiple buttons.</li>

</ul>

<div class="note" role="note" id="issue-container-generatedID-5"><div role="heading" class="note-title marker" id="h-note-5" aria-level="4"><span>Note</span></div><p class="">Users with cognitive and learning disabilities need to understand what items in a visual display are actionable targets and how to interact with them. There is a need for accessibility API's that map custom user interface actions to control types. These actions can then be understood by a broad range of assistive technologies. This would help indicate to users what targets are actionable, and how they can interact with them. By supporting this kind of adaptation and personalization the user can select preferred, familiar options from a set of alternatives. The <abbr title="World Wide Web Consortium">W3C</abbr> have produced a useful list of these patterns that could help readers understand the user needs of people with cognitive and learning disabilities, as well as in the development of suitable APIs. [<cite><a class="bibref" data-link-type="biblio" href="#bib-coga-usable" title="Making Content Usable for People with Cognitive and Learning Disabilities">coga-usable</a></cite>], especially <a href="https://www.w3.org/TR/coga-usable/#design_for_everyone">section 4, the Design Guide</a>.</p></div>



</section>

<section id="voice-commands"><div class="header-wrapper"><h3 id="x4-5-voice-commands"><bdi class="secno">4.5 </bdi>Voice commands</h3><a class="self-link" href="#voice-commands" aria-label="Permalink for Section 4.5"></a></div>



<ul>
<li><strong>User Need 5:</strong> A user with limited mobility may want to be able to use Voice Commands within the immersive environment, to navigate, interact and communicate with others.</li>

<li><strong>REQ 5a:</strong> Ensure Navigation and interaction can be controlled by Voice Activation.</li>
<li><strong>REQ 5b:</strong> Voice activation should preferably use native screen readers or voice assistants rather than external devices to eliminate the additional step needed to pair devices.</li>

</ul>

</section>

<section id="color-changes"><div class="header-wrapper"><h3 id="x4-6-color-changes"><bdi class="secno">4.6 </bdi>Color changes</h3><a class="self-link" href="#color-changes" aria-label="Permalink for Section 4.6"></a></div>



<ul>
<li><strong>User Need 6:</strong> Color blind users may need to be able to customise the colors used in the immersive environment. This will help with understanding affordances of various controls or where color is used to signify danger or permission.</li>

<li><strong>REQ 6a:</strong> Provide customised high contrast skins for the environment to suit luminosity and color contrast requirements.</li>

</ul>

</section>

<section id="magnification-context-and-resetting"><div class="header-wrapper"><h3 id="x4-7-magnification-context-and-resetting"><bdi class="secno">4.7 </bdi>Magnification context and resetting</h3><a class="self-link" href="#magnification-context-and-resetting" aria-label="Permalink for Section 4.7"></a></div>



<ul>
<li><strong>User Need 7:</strong> Screen magnification users may need to be able to check the context of their view&nbsp;in immersive environments.</li>

<li><strong>REQ 7a:</strong> Allow the screen magnification user to check the context of their view and track/reset focus as needed.</li>
<li><strong>REQ 7b:</strong> Where it makes sense (such as in menus) interface elements can be enlarged and the menu reflowed to enhance the usability of the interface up to a certain magnification requirement.</li>

</ul>

	<div class="note" role="note" id="issue-container-generatedID-6"><div role="heading" class="note-title marker" id="h-note-6" aria-level="4"><span>Note</span></div><p class="">There are customisation approaches such as the automatic generation of user interfaces as demonstrated in the SUPPLE project, which adapt to the different challenges the user may face, such as vision, motor control and other user preferences and abilities. A generated UI can make multiple adaptations for different user needs at the same time. This is achieved by generating a UI, or several - after testing a person's ability using an algorithm to learn their preferences. [<cite><a class="bibref" data-link-type="biblio" href="#bib-supple-project" title="SUPPLE: Automatically Generating Personalized User Interfaces">supple-project</a></cite>]</p></div>
	
</section>

<section id="critical-messaging-and-alerts"><div class="header-wrapper"><h3 id="x4-8-critical-messaging-and-alerts"><bdi class="secno">4.8 </bdi>Critical messaging and alerts</h3><a class="self-link" href="#critical-messaging-and-alerts" aria-label="Permalink for Section 4.8"></a></div>



<ul>

<li><strong>User Need 8:</strong> Screen magnification users may need to be made aware of critical messaging and alerts&nbsp;in immersive environments often without losing focus. They may also need to route these messages to a 'second screen' (see <strong>REQ 14</strong> Second Screen).</li>

<li><strong>REQ 8a:</strong> Ensure that critical messaging, or alerts have priority roles that can be understood and flagged to AT, without moving focus.</li>

</ul>

</section>

<section id="gestural-interfaces-and-interactions"><div class="header-wrapper"><h3 id="x4-9-gestural-interfaces-and-interactions"><bdi class="secno">4.9 </bdi>Gestural interfaces and interactions</h3><a class="self-link" href="#gestural-interfaces-and-interactions" aria-label="Permalink for Section 4.9"></a></div>



<ul>
<li><strong>User Need 9:</strong> A blind user may wish to interact with a gestural interface, such as a virtual menu system.</li>

<li><strong>REQ 9a:</strong> Support touch screen accessibility gestures (e.g. swipes, flicks and single, double or triple taps with 1, 2 or 3 fingers). See <strong>REQ 14</strong> Second Screen.</li>

<li><strong>REQ 9b:</strong> Using a virtual menu system -  enable a self-voicing option and have each category, or item description, spoken as they receive focus via a gesture or other input. As the blind user gestures to trigger both movement and interaction they may get more detail about items that are closer to them. The user must be allowed to query and interrogate these items and make selections.</li>

<li><strong>REQ 9c:</strong> Allow for the re-mapping of gestures to associate different actions with different input types or gestures. This may be a virtual switch that can map to new macros on the fly. This will allow the user to change defaults and employ gestures to carry out new actions offered by the immersive environment as required.</li>


</ul>

</section>

<section id="signing-videos-and-text-description-transformation"><div class="header-wrapper"><h3 id="x4-10-signing-videos-and-text-description-transformation"><bdi class="secno">4.10 </bdi>Signing videos and text description transformation</h3><a class="self-link" href="#signing-videos-and-text-description-transformation" aria-label="Permalink for Section 4.10"></a></div>



<ul>

<li><strong>User Need 10:</strong> A deaf or hard of hearing person, for whom a written language may not be their first language, may prefer signing of video for text, objects or item descriptions.</li>

<li><strong>REQ 10a:</strong> Allow text, objects or item descriptions to be presented to the user via a signing avatar (pre-recorded only).</li>

<li><strong>REQ 10b:</strong> Any signing videos should be 1/3rd minimum of the original streams signing size. This requirement comes from research of 240 ASL videos intended to be watched by deaf signers,
that found that nearly all set signer size to at least one-third the size of the full video. [<cite><a class="bibref" data-link-type="biblio" href="#bib-raja-asl" title="Legibility of Videos with ASL signers">raja-asl</a></cite>]</li>

</ul>

<div class="note" role="note" id="issue-container-generatedID-7"><div role="heading" class="note-title marker" id="h-note-7" aria-level="4"><span>Note</span></div><aside class="">
<p>
Currently, it is not possible to provide an accurate live interpretation via a signing avatar. In general, animated or digital signing avatars should be avoided as users find them less expressive than recorded video of humans who can convey the natural quality and skill provided by appropriately trained and qualified interpreters and translators.  Therefore, uses of signed avatars should rely only on pre-recording of 'real people' who are trained and qualified interpreters and translators. See the concerns expressed by the WFD and WASLI 'Statement on Use of Signing Avatars'. [<cite><a class="bibref" data-link-type="biblio" href="#bib-wfd-wasli" title="WFD and WASLI">wfd-wasli</a></cite>] </p>
<p>However, we note this is an emerging field and exploration is encouraged to ensure the future development of quality signing avatars. For example, this could be via building a signing avatar that both provides a face with fully functioning muscular variables and can successfully parse the nuances of vocal expression and meaning.</p>
</aside></div>

</section>

<section id="safe-harbour-controls"><div class="header-wrapper"><h3 id="x4-11-safe-harbour-controls"><bdi class="secno">4.11 </bdi>Safe harbour controls</h3><a class="self-link" href="#safe-harbour-controls" aria-label="Permalink for Section 4.11"></a></div>



<ul>
<li><strong>User Need 11:</strong> People with Cognitive Impairments may be easily overwhelmed in Immersive Environments.</li>

<li><strong>REQ 11a:</strong> Allow the user to set a 'safe place' - quick key, shortcut or macro.</li>

</ul>

</section>

<section id="immersive-time-limits"><div class="header-wrapper"><h3 id="x4-12-immersive-time-limits"><bdi class="secno">4.12 </bdi>Immersive time limits</h3><a class="self-link" href="#immersive-time-limits" aria-label="Permalink for Section 4.12"></a></div>



<ul>

<li><strong>User Need 12:</strong> Users may be adversely affected by spending too much time in an immersive environment or experience, and may lose track of time. </li>

<li><strong>REQ 12a:</strong>  Provide a platform integration with tools that support digital wellbeing, allow the user to access alarms for time limits during an immersive session. </li>

</ul>

</section>

<section id="orientation-and-navigation"><div class="header-wrapper"><h3 id="x4-13-orientation-and-navigation"><bdi class="secno">4.13 </bdi>Orientation and navigation</h3><a class="self-link" href="#orientation-and-navigation" aria-label="Permalink for Section 4.13"></a></div>



<ul>

<li><strong>User Need 13:</strong> A screen magnification user or user with a cognitive and learning disability or spatial orientation impairment needs to maintain focus and understand where they are in immersive environments.</li>

<li><strong>REQ 13a:</strong> Ensure the user can reset and calibrate their orientation/view in a device independent way.</li>

<li><strong>REQ 13b:</strong> Ensure field of view in Immersive environments, are appropriate, and can be personalized - so users are not disorientated.</li>
<li><strong>REQ 13c:</strong> Provide clear visual or audio landmarks.</li>

</ul>

</section>

<section id="second-screen-devices"><div class="header-wrapper"><h3 id="x4-14-second-screen-devices"><bdi class="secno">4.14 </bdi>Second screen devices</h3><a class="self-link" href="#second-screen-devices" aria-label="Permalink for Section 4.14"></a></div>



<ul>

<li><strong>User Need 14:</strong> Users of assistive technology such as a blind, or deaf-blind users communicating via a RTC application in <abbr title="Virtual and Augmented Reality">XR</abbr>, may have sophisticated 'routing' requirements for various inputs and outputs and the need to manage same.&nbsp;</li>

<li><strong>REQ 14a:</strong> Allow the user to route text output, alerts, environment sounds or audio to a braille or other second screen device.</li>
<li><strong>REQ 14b:</strong> Ensure that the user can manage the flow of critical messaging, or content to display on a second screen.</li>
<li><strong>REQ 14c:</strong> Support touch screen accessibility gestures (e.g. swipes, flicks and single, double or triple taps with 1, 2 or 3 fingers) on a second screen device to allow the user to navigate menus and interact.</li>

</ul>

<div class="note" role="note" id="issue-container-generatedID-8"><div role="heading" class="note-title marker" id="h-note-8" aria-level="4"><span>Note</span></div><p class="">'Second screen' is a term used in this document to denote any another external output device, such as a monitor or sound card, or assistive technology such as braille output. The use of the term is not restricted to just these devices and can refer to any output device a user may choose.</p></div>

</section>

<section id="interaction-speed"><div class="header-wrapper"><h3 id="x4-15-interaction-speed"><bdi class="secno">4.15 </bdi>Interaction speed</h3><a class="self-link" href="#interaction-speed" aria-label="Permalink for Section 4.15"></a></div>



<ul>
<li><strong>User Need 15:</strong> Users with physical disabilities or cognitive and learning disabilities may find some interactions too fast to keep up with or maintain.</li>

<li><strong>REQ 15a:</strong> Allow users to change the speed they can travel or perform interactions, in an immersive environment.</li>

<li><strong>REQ 15b:</strong> Allow timings for interactions or critical inputs to be modified or extended.</li>

<li><strong>REQ 15c:</strong> Provide help for the user with a cognitive or learning disability.</li>

<li><strong>REQ 15d:</strong> Provide clear start and stop mechanisms.</li>

</ul>

<div class="note" role="note" id="issue-container-generatedID-9"><div role="heading" class="note-title marker" id="h-note-9" aria-level="4"><span>Note</span></div><p class=""> The term 'help' for REQ 15c may vary from explanatory information such as textual/symbolic annotations in an application, to human assistance in real time.</p></div>

</section>

<section id="avoiding-sickness-triggers"><div class="header-wrapper"><h3 id="x4-16-avoiding-sickness-triggers"><bdi class="secno">4.16 </bdi>Avoiding sickness triggers</h3><a class="self-link" href="#avoiding-sickness-triggers" aria-label="Permalink for Section 4.16"></a></div>



<ul>
<li><strong>User Need 16:</strong> Users with vestibular disorders, Epilepsy, and photo sensitivity may find some interactions trigger motion sickness and other affects. This may be triggered when doing teleportation or other movements in <abbr title="Virtual and Augmented Reality">XR</abbr>.</li>

<li><strong>REQ 16a:</strong> Avoid interactions that trigger epilepsy or motion sickness and provide alternatives.</li>
<li><strong>REQ 16b:</strong> Ensure flickering images are at a minimum, will not trigger seizures (more than 3 times a second), or can be turned off or reduced.</li>

</ul>

</section>

<section id="spatial-audio-tracks-and-alternatives"><div class="header-wrapper"><h3 id="x4-17-spatial-audio-tracks-and-alternatives"><bdi class="secno">4.17 </bdi>Spatial audio tracks and alternatives</h3><a class="self-link" href="#spatial-audio-tracks-and-alternatives" aria-label="Permalink for Section 4.17"></a></div>



<ul>
<li><strong>User Need 17:</strong> Hard of hearing users may need accommodations to perceive audio.</li>

<li><strong>REQ 17a:</strong> Provide spatialized audio content to emulate three-dimensional sound forms in immersive environments.</li>
<li><strong>REQ 17b:</strong> Provide text descriptions of important audio content.</li>

</ul>

</section>

	<section id="spatial-orientation-mono-audio-option"><div class="header-wrapper"><h3 id="x4-18-spatial-orientation-mono-audio-option"><bdi class="secno">4.18 </bdi>Spatial orientation: Mono audio option</h3><a class="self-link" href="#spatial-orientation-mono-audio-option" aria-label="Permalink for Section 4.18"></a></div>


<ul>

<li><strong>User Need 18:</strong> Users with spatial orientation impairments, cognitive impairments or hearing loss in just one ear may miss information in a stereo or binaural soundscape.</li>

<li><strong>REQ 18a:</strong> Allow mono audio sound to be sent to both headphones so that the user can perceive the whole soundscape through either ear. [<cite><a class="bibref" data-link-type="biblio" href="#bib-mono-ios" title="iPhone User Guide">mono-ios</a></cite>].</li>

</ul>

<div class="note" role="note" id="issue-container-generatedID-10"><div role="heading" class="note-title marker" id="h-note-10" aria-level="4"><span>Note</span></div><p class="">People with traumatic brain injuries can have a range of impairments. These may be spatial orientation impairments, auditory processing difficulties, visual processing difficulties or a combination. They may miss information in stereo or binaural soundscapes. This can affect orientation while navigating. Even if provided with accurate directions, they may not recognize surroundings, or experience anxiety when navigating. </p></div>

	</section>

	<section id="captioning-subtitling-and-text-support-and-customization"><div class="header-wrapper"><h3 id="x4-19-captioning-subtitling-and-text-support-and-customization"><bdi class="secno">4.19 </bdi>Captioning, Subtitling and Text: Support and customization</h3><a class="self-link" href="#captioning-subtitling-and-text-support-and-customization" aria-label="Permalink for Section 4.19"></a></div>


<ul>

<li><strong>User Need 19:</strong> Users may need to customise captions, subtitles and other text in <abbr title="Virtual and Augmented Reality">XR</abbr> environments.</li>


<li><strong>REQ 19a:</strong> Provide support for captioning and subtitling of multimedia content.</li>

<li><strong>REQ 19b:</strong> Allow customisable context sensitive reflow of captions, subtitles and text content in <abbr title="Virtual and Augmented Reality">XR</abbr> environments. The suitable subtitling area may be smaller than what is required currently for television. [<cite><a class="bibref" data-link-type="biblio" href="#bib-inclusive-seattle" title="W3C Workshop on Inclusive XR Seattle">inclusive-seattle</a></cite>]</li>

</ul>

<div class="note" role="note" id="issue-container-generatedID-11"><div role="heading" class="note-title marker" id="h-note-11" aria-level="4"><span>Note</span></div><p class="">The <a href="https://www.w3.org/community/immersive-captions/"><abbr title="World Wide Web Consortium">W3C</abbr> Immersive Captions Community Group</a> is actively contributing to this emerging accessibility standards work representing a diverse range of user needs.</p></div>

	</section>
</section>




<section id="related-documents"><div class="header-wrapper"><h2 id="x5-related-documents"><bdi class="secno">5. </bdi>Related Documents</h2><a class="self-link" href="#related-documents" aria-label="Permalink for Section 5."></a></div>

<p>Other documents that relate to this and represent current work in the RQTF/APA are:</p>
<ul>
<li> <a href="https://www.w3.org/WAI/APA/wiki/XRA-Semantics-Module"><abbr title="Virtual and Augmented Reality">XR</abbr> Semantics Module</a> - this document outlines proposed accessibility requirements that may be used in a modular way in immersive, augmented or mixed reality (<abbr title="Virtual and Augmented Reality">XR</abbr>).  A modular approach may help us to define clear accessibility requirements that support <abbr title="Virtual and Augmented Reality">XR</abbr> accessibility user needs, as they relate to the immersive environment, objects, movement, and interaction accessibility. Such a modular approach may help the development of clear semantics, designed to describe specific parts of the immersive eco-system. In immersive environments it is imperative  that the user can understand what objects are, understand their purpose, as well as another qualities and properties including interaction affordance, size, form, shape, and other inherent properties or attributes.</li>

<li><a href="https://www.w3.org/WAI/APA/wiki/WebXR_Standards_and_Accessibility_Architecture_Issues"> WebXR Standards and Accessibility Architecture Issues</a> - this document is informative and aims to outline some of the challenges in understanding the complex technical architecture and processes behind how <abbr title="Virtual and Augmented Reality">XR</abbr> environments are currently rendered. To make these environments accessible and provide a quality user experience it is important to also understand the nuances and complexity of accessible user interface design and development for the 2D web. Any attempt to make <abbr title="Virtual and Augmented Reality">XR</abbr> accessible needs to be based on meeting the practical user needs of people with disabilities.</li>
</ul>
</section>


<section class="appendix" id="change-log"><div class="header-wrapper"><h2 id="c-change-log"><bdi class="secno">A. </bdi>Change Log<a class="self-link" aria-label="§" href="#change-log"></a></h2><a class="self-link" href="#c-change-log" aria-label="Permalink for Appendix A."></a></div>
			


<p>The following is a list of new requirements and other changes in this document:</p>

<ul>
	<li><strong>Immersive semantics and customization:</strong> 
	REQ  1c:  Objects that are important within any given context of time and place can be identified in a suitable modality.</li>
	<li><strong>Immersive semantics and customization:</strong>  
	REQ 1d:  Allow filtering and the ability to query items and content for more details. </li>
	<li><strong>REQ 1e:</strong> Allow the user to query objects and content for more details.</li>
	
	<li> <strong>Mono audio option:</strong> 
	REQ 19a: Allow mono audio sound to be sent to both headphones so that the user can perceive the whole soundscape through either ear. [<cite><a class="bibref" data-link-type="biblio" href="#bib-mono-ios" title="iPhone User Guide">mono-ios</a></cite>]</li>
	
	<li> <strong>Interaction and target customization:</strong> 
	REQ 4d: Support 'Sticky Keys' requirements such as serialization for various inputs when the user needs to press multiple buttons.</li>
	
	<li> <strong>Gestural interfaces and interactions</strong>
	 REQ 9a: Support touch screen accessibility gestures (e.g. swipes, flicks and single, double or triple taps with 1, 2 or 3 fingers).</li>
	
	<li> <strong>Magnification context and resetting </strong>
	REQ 7b: Where it makes sense (such as in menus) interface elements can be enlarged and the menu reflowed to enhance the usability of the interface up to a certain magnification requirement.</li>
	
	<li> <strong>Gestural interfaces and interactions</strong>
	
	REQ 9a: Support touch screen accessibility gestures (e.g. swipes, flicks and single, double or triple taps with 1, 2 or 3 fingers) on a second screen device to navigate menus and interact. </li>
	
	<li> <strong>Gestural interfaces and interactions</strong>
	REQ 9c: Allow for the re-mapping of gestures to associate different actions with different input types or gestures. This may be a virtual switch that can map to new macros on the fly. This will allow the user to change defaults and employ gestures to carry out new actions offered by the immersive environment as required.</li>

	<li><strong>Signing videos and text description transformation</strong> REQ 10b: Any signing videos should be 1/3rd minimum of the original streams signing size. This requirement comes from research of 240 ASL videos intended to be watched by deaf signers, that found that nearly all set signer size to at least one-third the size of the full video. </li>

	<li> <strong>Orientation and navigation</strong> REQ 13c: Provide clear visual or audio landmarks.</li>
	
	<li> <strong>Second screen devices</strong>
	REQ 14b: Ensure that the user can manage the flow of critical messaging, or content to display on a second screen.</li>
	<li> <strong>Second screen</strong> REQ 14c: Support touch screen accessibility gestures (e.g. swipes, flicks and single, double or triple taps with 1, 2 or 3 fingers) on a second screen device to allow the user to navigate menus and interact.</li>
	<li><strong>Spatial audio tracks and alternatives</strong> REQ 17b: Provide text descriptions of important audio content.</li>
</ul>

	
<p>Requirements have been updated based on combined review feedback, discussion and Research Questions Task Force consensus. Other user needs have been edited to better reference related requirements such as with Second screen devices.</p>
<p>Various clarification or reference notes have been added relating to: </p> 

<ul>
<li>The importance of discoverability relating to accessibility features in <abbr title="Virtual and Augmented Reality">XR</abbr>.</li> 
<li>Specific augmented reality issues when marking locations in a motion agnostic way.</li>
<li><abbr title="World Wide Web Consortium">W3C</abbr> Personalization semantics and how they can support people with cognitive impairments.</li>
<li>The need for accessibility API's that map custom user interface actions to control types.</li>
<li>The limitations of accurate live interpretation via a digital signing avatar.</li>
<li>The impact of traumatic brain injury on a user's spatial orientation, auditory and visual processing abilities.</li>
</ul>
		</section>

<section class="appendix" id="acknowledgements"><div class="header-wrapper"><h2 id="b-acknowledgements"><bdi class="secno">B. </bdi>Acknowledgements</h2><a class="self-link" href="#acknowledgements" aria-label="Permalink for Appendix B."></a></div>

	<section id="participants-of-the-apa-working-group-active-in-the-development-of-this-document"><div class="header-wrapper"><h3 id="b-1-participants-of-the-apa-working-group-active-in-the-development-of-this-document"><bdi class="secno">B.1 </bdi>Participants of the APA working group active in the development of this document</h3><a class="self-link" href="#participants-of-the-apa-working-group-active-in-the-development-of-this-document" aria-label="Permalink for Appendix B.1"></a></div>
		
	<ul>
		<li>Shadi Abou-Zahra, <abbr title="World Wide Web Consortium">W3C</abbr></li>
		<li>Matthew Tylee Atkinson, The Paciello Group</li>
		<li>Judy Brewer, <abbr title="World Wide Web Consortium">W3C</abbr></li>
		<li>Michael Cooper, <abbr title="World Wide Web Consortium">W3C</abbr></li>
		<li>Scott Hollier, Invited Expert</li>
		<li>Joshue O'Connor, <abbr title="World Wide Web Consortium">W3C</abbr></li>
		<li>Janina Sajka, Invited Expert</li>
		<li>Jason White, Educational Testing Service</li>
	</ul>
	</section>
	<section id="previously-active-participants-commenters-and-other-contributors"><div class="header-wrapper"><h3 id="b-2-previously-active-participants-commenters-and-other-contributors"><bdi class="secno">B.2 </bdi>Previously active participants, commenters, and other contributors</h3><a class="self-link" href="#previously-active-participants-commenters-and-other-contributors" aria-label="Permalink for Appendix B.2"></a></div>
		
		<ul>
		<li>Frances Baum</li>
		<li>Nicoló Carpignoli, Chialab </li>
		<li>Wendy Dannels, RIT/NTID Research Center on Culture and Language</li>
		<li>David Fazio, Helix Opportunity</li>
		<li>Marku Hakkinen, Educational Testing Service</li>
		<li>Charles Hall, Invited Expert</li>
		<li>Ian Hamilton</li>
		<li>John Kirkwood</li>
		<li>Raja Kushalnagar, Department of Science, Technology and Mathematics Gallaudet University</li>
		<li>Charles LaPierre, Benetech</li>
		<li>Thomas Logan, Equal Entry</li>
		<li>Melina Möhlne, IRT</li>
		<li>Estel·la Oncins Noguer, UAB TransMedia Catalonia</li>
		<li>Christopher Patnoe, Google</li>
		<li>Devon Persing, Shopify</li>
		<li>Sonali Rai, Royal National Institute of the Blind</li>
		<li>Ajay Sharma, HCL Technologies</li>
		<li>Suzanne Taylor, Things Entertainment</li>
		<li>Léonie Watson, TetraLogical</li>
	</ul>
	</section>
	<section id="enabling-funders"><div class="header-wrapper"><h3 id="b-3-enabling-funders"><bdi class="secno">B.3 </bdi>Enabling Funders</h3><a class="self-link" href="#enabling-funders" aria-label="Permalink for Appendix B.3"></a></div>

<p>This work is supported by the <a href="https://www.w3.org/WAI/about/projects/wai-guide/">EC-funded WAI-Guide Project</a>.</p>
	</section>
</section>




<section id="references" class="appendix"><div class="header-wrapper"><h2 id="c-references"><bdi class="secno">C. </bdi>References</h2><a class="self-link" href="#references" aria-label="Permalink for Appendix C."></a></div><section id="informative-references"><div class="header-wrapper"><h3 id="c-1-informative-references"><bdi class="secno">C.1 </bdi>Informative references</h3><a class="self-link" href="#informative-references" aria-label="Permalink for Appendix C.1"></a></div>
    
    <dl class="bibliography"><dt id="bib-able-gamers">[able-gamers]</dt><dd>
      <a href="https://ablegamers.org/thoughts-on-accessibility-and-vr/"><cite>Thought On Accessibility and VR</cite></a>. AJ Ryan. March, 2017. URL: <a href="https://ablegamers.org/thoughts-on-accessibility-and-vr/">https://ablegamers.org/thoughts-on-accessibility-and-vr/</a>
    </dd><dt id="bib-coga-usable">[coga-usable]</dt><dd>
      <a href="https://www.w3.org/TR/coga-usable/"><cite>Making Content Usable for People with Cognitive and Learning Disabilities</cite></a>. Lisa Seeman-Horwitz; Rachael Bradley Montgomery; Steve Lee; Ruoxi Ran.  W3C. 29 April 2021. W3C Working Group Note. URL: <a href="https://www.w3.org/TR/coga-usable/">https://www.w3.org/TR/coga-usable/</a>
    </dd><dt id="bib-game-a11y">[game-a11y]</dt><dd>
      <a href="http://gameaccessibilityguidelines.com"><cite>Game Accessibility Guidelines</cite></a>. Barrie Ellis; Ian Hamilton; Gareth Ford-Williams; Lynsey Graham; Dimitris Grammenos; Ed Lee; Jake Manion; Thomas Westin. 2019. URL: <a href="http://gameaccessibilityguidelines.com">http://gameaccessibilityguidelines.com</a>
    </dd><dt id="bib-inclusive-seattle">[inclusive-seattle]</dt><dd>
      <a href="https://www.w3.org/2019/08/inclusive-xr-workshop/"><cite>W3C Workshop on Inclusive XR Seattle</cite></a>. W3C; Pluto VR.  W3C. Nov 2019. URL: <a href="https://www.w3.org/2019/08/inclusive-xr-workshop/">https://www.w3.org/2019/08/inclusive-xr-workshop/</a>
    </dd><dt id="bib-maidenbaum-amendi">[maidenbaum-amendi]</dt><dd>
      <cite>Non-visual virtual interaction: Can Sensory Substitution generically increase the accessibility of Graphical virtual reality to the blind?</cite>. Maidenbaum, S.; Amedi, A.  In Virtual and Augmented Assistive Technology (VAAT), 2015 3rd IEEE VR International Workshop on (pp. 15-17). IEEE. 2015. 
    </dd><dt id="bib-mono-ios">[mono-ios]</dt><dd>
      <a href="https://support.apple.com/en-gb/guide/iphone/iph3e2e2cdc/ios"><cite>iPhone User Guide</cite></a>. Apple. 2020. URL: <a href="https://support.apple.com/en-gb/guide/iphone/iph3e2e2cdc/ios">https://support.apple.com/en-gb/guide/iphone/iph3e2e2cdc/ios</a>
    </dd><dt id="bib-personalization-content">[personalization-content]</dt><dd>
      <a href="https://www.w3.org/TR/personalization-semantics-content-1.0/"><cite>Personalization Semantics Content Module 1.0</cite></a>. Lisa Seeman; Charles LaPierre; Michael Cooper; Roy Ran; Richard Schwerdtfeger.  W3C. 2020. URL: <a href="https://www.w3.org/TR/personalization-semantics-content-1.0/">https://www.w3.org/TR/personalization-semantics-content-1.0/</a>
    </dd><dt id="bib-personalization-requirements">[personalization-requirements]</dt><dd>
      <a href="https://www.w3.org/TR/personalization-semantics-requirements-1.0/"><cite>Requirements for Personalization Semantics</cite></a>. Lisa Seeman; Charles LaPierre; Michael Cooper; Roy Ran.  W3C. 2020. URL: <a href="https://www.w3.org/TR/personalization-semantics-requirements-1.0/">https://www.w3.org/TR/personalization-semantics-requirements-1.0/</a>
    </dd><dt id="bib-personalization-semantics">[personalization-semantics]</dt><dd>
      <a href="https://www.w3.org/TR/personalization-semantics-1.0/"><cite>Personalization Semantics Explainer 1.0</cite></a>. Lisa Seeman; Charles LaPierre; Michael Cooper; Roy Ran; Richard Schwerdtfeger.  W3C. 2020. URL: <a href="https://www.w3.org/TR/personalization-semantics-1.0/">https://www.w3.org/TR/personalization-semantics-1.0/</a>
    </dd><dt id="bib-raja-asl">[raja-asl]</dt><dd>
      <a href="https://arxiv.org/abs/2105.12928"><cite>Legibility of Videos with ASL signers</cite></a>.  Cornell UNiversity. 27 May 2021. URL: <a href="https://arxiv.org/abs/2105.12928">https://arxiv.org/abs/2105.12928</a>
    </dd><dt id="bib-spatialized-navigation">[spatialized-navigation]</dt><dd>
      <a href="https://link.springer.com/chapter/10.1007/978-3-642-30973-1_5"><cite>What’s around Me? Spatialized Audio Augmented Reality for Blind Users with a Smartphone</cite></a>. Blum J.R.; Bouchard M.; Cooperstock J.R. .  Springer. 2012. URL: <a href="https://link.springer.com/chapter/10.1007/978-3-642-30973-1_5">https://link.springer.com/chapter/10.1007/978-3-642-30973-1_5</a>
    </dd><dt id="bib-supple-project">[supple-project]</dt><dd>
      <a href="http://www.eecs.harvard.edu/~kgajos/research/supple/"><cite>SUPPLE: Automatically Generating Personalized User Interfaces</cite></a>. Krzysztof Gajos et al.  Harvard. 2010. URL: <a href="http://www.eecs.harvard.edu/~kgajos/research/supple/">http://www.eecs.harvard.edu/~kgajos/research/supple/</a>
    </dd><dt id="bib-web-adapt">[web-adapt]</dt><dd>
      <a href="https://www.w3.org/2018/12/games-workshop/papers/web-games-adaptive-accessibility.html"><cite>W3C Workshop on Web Games Position Paper: Adaptive Accessibility</cite></a>. Matthew Tylee Atkinson; Ian Hamilton; Joe Humbert; Kit Wessendorf.  W3C. Dec 2018. URL: <a href="https://www.w3.org/2018/12/games-workshop/papers/web-games-adaptive-accessibility.html">https://www.w3.org/2018/12/games-workshop/papers/web-games-adaptive-accessibility.html</a>
    </dd><dt id="bib-wfd-wasli">[wfd-wasli]</dt><dd>
      <a href="https://wfdeaf.org/news/resources/wfd-wasli-statement-use-signing-avatars/"><cite>WFD and WASLI</cite></a>.  World Federation of the Deaf. 14 March 2081. URL: <a href="https://wfdeaf.org/news/resources/wfd-wasli-statement-use-signing-avatars/">https://wfdeaf.org/news/resources/wfd-wasli-statement-use-signing-avatars/</a>
    </dd></dl>
  </section></section><p role="navigation" id="back-to-top">
    <a href="#title"><abbr title="Back to Top">↑</abbr></a>
  </p><script id="respec-dfn-panel">(() => {
// @ts-check
if (document.respec) {
  document.respec.ready.then(setupPanel);
} else {
  setupPanel();
}

function setupPanel() {
  const listener = panelListener();
  document.body.addEventListener("keydown", listener);
  document.body.addEventListener("click", listener);
}

function panelListener() {
  /** @type {HTMLElement} */
  let panel = null;
  return event => {
    const { target, type } = event;

    if (!(target instanceof HTMLElement)) return;

    // For keys, we only care about Enter key to activate the panel
    // otherwise it's activated via a click.
    if (type === "keydown" && event.key !== "Enter") return;

    const action = deriveAction(event);

    switch (action) {
      case "show": {
        hidePanel(panel);
        /** @type {HTMLElement} */
        const dfn = target.closest("dfn, .index-term");
        panel = document.getElementById(`dfn-panel-for-${dfn.id}`);
        const coords = deriveCoordinates(event);
        displayPanel(dfn, panel, coords);
        break;
      }
      case "dock": {
        panel.style.left = null;
        panel.style.top = null;
        panel.classList.add("docked");
        break;
      }
      case "hide": {
        hidePanel(panel);
        panel = null;
        break;
      }
    }
  };
}

/**
 * @param {MouseEvent|KeyboardEvent} event
 */
function deriveCoordinates(event) {
  const target = /** @type HTMLElement */ (event.target);

  // We prevent synthetic AT clicks from putting
  // the dialog in a weird place. The AT events sometimes
  // lack coordinates, so they have clientX/Y = 0
  const rect = target.getBoundingClientRect();
  if (
    event instanceof MouseEvent &&
    event.clientX >= rect.left &&
    event.clientY >= rect.top
  ) {
    // The event probably happened inside the bounding rect...
    return { x: event.clientX, y: event.clientY };
  }

  // Offset to the middle of the element
  const x = rect.x + rect.width / 2;
  // Placed at the bottom of the element
  const y = rect.y + rect.height;
  return { x, y };
}

/**
 * @param {Event} event
 */
function deriveAction(event) {
  const target = /** @type {HTMLElement} */ (event.target);
  const hitALink = !!target.closest("a");
  if (target.closest("dfn:not([data-cite]), .index-term")) {
    return hitALink ? "none" : "show";
  }
  if (target.closest(".dfn-panel")) {
    if (hitALink) {
      return target.classList.contains("self-link") ? "hide" : "dock";
    }
    const panel = target.closest(".dfn-panel");
    return panel.classList.contains("docked") ? "hide" : "none";
  }
  if (document.querySelector(".dfn-panel:not([hidden])")) {
    return "hide";
  }
  return "none";
}

/**
 * @param {HTMLElement} dfn
 * @param {HTMLElement} panel
 * @param {{ x: number, y: number }} clickPosition
 */
function displayPanel(dfn, panel, { x, y }) {
  panel.hidden = false;
  // distance (px) between edge of panel and the pointing triangle (caret)
  const MARGIN = 20;

  const dfnRects = dfn.getClientRects();
  // Find the `top` offset when the `dfn` can be spread across multiple lines
  let closestTop = 0;
  let minDiff = Infinity;
  for (const rect of dfnRects) {
    const { top, bottom } = rect;
    const diffFromClickY = Math.abs((top + bottom) / 2 - y);
    if (diffFromClickY < minDiff) {
      minDiff = diffFromClickY;
      closestTop = top;
    }
  }

  const top = window.scrollY + closestTop + dfnRects[0].height;
  const left = x - MARGIN;
  panel.style.left = `${left}px`;
  panel.style.top = `${top}px`;

  // Find if the panel is flowing out of the window
  const panelRect = panel.getBoundingClientRect();
  const SCREEN_WIDTH = Math.min(window.innerWidth, window.screen.width);
  if (panelRect.right > SCREEN_WIDTH) {
    const newLeft = Math.max(MARGIN, x + MARGIN - panelRect.width);
    const newCaretOffset = left - newLeft;
    panel.style.left = `${newLeft}px`;
    /** @type {HTMLElement} */
    const caret = panel.querySelector(".caret");
    caret.style.left = `${newCaretOffset}px`;
  }

  // As it's a dialog, we trap focus.
  // TODO: when <dialog> becomes a implemented, we should really
  // use that.
  trapFocus(panel, dfn);
}

/**
 * @param {HTMLElement} panel
 * @param {HTMLElement} dfn
 * @returns
 */
function trapFocus(panel, dfn) {
  /** @type NodeListOf<HTMLAnchorElement> elements */
  const anchors = panel.querySelectorAll("a[href]");
  // No need to trap focus
  if (!anchors.length) return;

  // Move focus to first anchor element
  const first = anchors.item(0);
  first.focus();

  const trapListener = createTrapListener(anchors, panel, dfn);
  panel.addEventListener("keydown", trapListener);

  // Hiding the panel releases the trap
  const mo = new MutationObserver(records => {
    const [record] = records;
    const target = /** @type HTMLElement */ (record.target);
    if (target.hidden) {
      panel.removeEventListener("keydown", trapListener);
      mo.disconnect();
    }
  });
  mo.observe(panel, { attributes: true, attributeFilter: ["hidden"] });
}

/**
 *
 * @param {NodeListOf<HTMLAnchorElement>} anchors
 * @param {HTMLElement} panel
 * @param {HTMLElement} dfn
 * @returns
 */
function createTrapListener(anchors, panel, dfn) {
  const lastIndex = anchors.length - 1;
  let currentIndex = 0;
  return event => {
    switch (event.key) {
      // Hitting "Tab" traps us in a nice loop around elements.
      case "Tab": {
        event.preventDefault();
        currentIndex += event.shiftKey ? -1 : +1;
        if (currentIndex < 0) {
          currentIndex = lastIndex;
        } else if (currentIndex > lastIndex) {
          currentIndex = 0;
        }
        anchors.item(currentIndex).focus();
        break;
      }

      // Hitting "Enter" on an anchor releases the trap.
      case "Enter":
        hidePanel(panel);
        break;

      // Hitting "Escape" returns focus to dfn.
      case "Escape":
        hidePanel(panel);
        dfn.focus();
        return;
    }
  };
}

/** @param {HTMLElement} panel */
function hidePanel(panel) {
  if (!panel) return;
  panel.hidden = true;
  panel.classList.remove("docked");
}
})()</script><script src="https://www.w3.org/scripts/TR/2021/fixup.js"></script></body></html>